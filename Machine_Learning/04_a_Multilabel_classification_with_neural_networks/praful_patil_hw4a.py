# -*- coding: utf-8 -*-
"""Praful_Patil_HW4A.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Debh9Fydo7tWpkXPCiyPk_OoWig8R4Ys

###**Setting up the environment**
"""

# CHANGE FOLDERS AS PER YOUR SETUP
from pathlib import Path
if 'google.colab' in str(get_ipython()):
    from google.colab import drive
    drive.mount("/content/drive")
    !pip install datasets transformers evaluate wandb accelerate -U -qq
    base_folder = Path("/content/drive/MyDrive")
else:
    base_folder = Path("/home/harpreet/Insync/google_drive_shaannoor/data")

from sklearn.model_selection import train_test_split
import evaluate
import torch
from torch.utils.data import Dataset, DataLoader
import ast
import joblib
import torch.nn as nn
import numpy as np
from sklearn.preprocessing import MultiLabelBinarizer
!pip install torchmetrics
from torchmetrics import HammingDistance
import pandas as pd
from functools import partial

"""##**Loading Dataset**"""

base_folder = Path('/content/drive/MyDrive/NLP/HW_4')
data_folder = base_folder
custom_functions = base_folder/'custom-functions'

file_name = 'df_multilabel_hw_cleaned.joblib'
file_path = data_folder / file_name

# Load the dataset using joblib
df_multilabel = joblib.load(file_path)

df_multilabel.head(10)

# Convert the 'Tag_Number' column from string representations of lists to actual lists of integers
df_multilabel['Tag_Number'] = df_multilabel['Tag_Number'].apply(lambda x: ast.literal_eval(x))

# Initialize MultiLabelBinarizer
mlb = MultiLabelBinarizer()

# Fit and transform the 'Tag_Number' column to one-hot encoded format
one_hot_labels = mlb.fit_transform(df_multilabel['Tag_Number'])

# Convert one-hot encoded labels to a DataFrame
one_hot_labels_df = pd.DataFrame(one_hot_labels, columns=mlb.classes_)

# Concatenate the one-hot encoded labels DataFrame with the original DataFrame
df_multilabel = pd.concat([df_multilabel, one_hot_labels_df], axis=1)

df_multilabel.head(10)

"""##**Splitting the dataset**"""

X = df_multilabel['cleaned_text'].values
y = one_hot_labels

# Split the data into train, validation, and test sets (60%, 20%, 20%)
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

"""##**CustomDataset class for loading data and labels.**"""

class CustomDataset(Dataset):
    """
    Custom Dataset class for loading data and labels.
    """
    def __init__(self, X, y):
        self.X = X
        self.y = y

    def __len__(self):
        return len(self.X)

    def __getitem__(self, idx):
        texts = self.X[idx]
        labels = self.y[idx]
        sample = (labels, texts)
        return sample

train_dataset = CustomDataset(X_train, y_train)
val_dataset = CustomDataset(X_val, y_val)
test_dataset = CustomDataset(X_test, y_test)

from collections import Counter
from torchtext.vocab import vocab
!pip install torchinfo
from torchinfo import summary

def get_vocab(dataset, min_freq=1):
    """
    Generate a vocabulary from a dataset.

    Args:
        dataset (list of tuple): List of tuples where each tuple contains a label and a text.
        min_freq (int): The minimum frequency for a token to be included in the vocabulary.

    Returns:
        torchtext.vocab.Vocab: Vocabulary object.
    """
    # Initialize a counter object to hold token frequencies
    counter = Counter()

    # Update the counter with tokens from each text in the dataset
    for (label, text) in dataset:
        counter.update(text.split())

    # Create a vocabulary using the counter object
    # Tokens that appear fewer times than `min_freq` are excluded
    my_vocab = vocab(counter, min_freq=min_freq)

    # Insert a '<unk>' token at index 0 to represent unknown words
    my_vocab.insert_token('<unk>', 0)

    # Set the default index to 0
    # This ensures that any unknown word will be mapped to '<unk>'
    my_vocab.set_default_index(0)

    return my_vocab

vocab = get_vocab(train_dataset, min_freq=2)

len(vocab)

"""##**Collate_fn for Data Loaders**"""

# Creating a function that will be used to get the indices of words from vocab
def tokenizer(x, vocab):
    """Converts text to a list of indices using a vocabulary dictionary"""
    return [vocab[token] for token in x.split()]

def collate_batch(batch, my_vocab):
    labels, texts = zip(*batch)

    # Convert labels to tensor of dtype float
    labels = torch.tensor(np.array(labels), dtype=torch.float)  # Use np.array to avoid UserWarning

    # Convert list of texts to list of lists of indices
    list_of_list_of_indices = [tokenizer(text, my_vocab) for text in texts]

    # Efficiently concatenate lists and then convert to tensor
    concatenated_indices = [index for indices in list_of_list_of_indices for index in indices]
    indices = torch.tensor(concatenated_indices, dtype=torch.int64)

    # Compute offsets
    offsets = [0] + [len(indices) for indices in list_of_list_of_indices]
    # Use torch.cumsum on the list of lengths, not on the offsets list itself
    offsets = torch.tensor(offsets).cumsum(dim=0)[:-1]  # Exclude the last cumulative sum to match indices length

    return (indices, offsets), labels

# Define the batch size
batch_size = 128

train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)
val_loader = DataLoader(dataset=val_dataset, batch_size=batch_size, shuffle=False)
test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)

"""##**Creating custom model class**"""

class SimpleMLP(nn.Module):
    def __init__(self, vocab_size, embedding_dim, hidden_dim1, hidden_dim2, drop_prob1, drop_prob2, num_outputs):
        super().__init__()

        # EmbeddingBag_layer->Linear->ReLU->BatchNorm->Dropout->Linear->ReLU->BatchNorm->Dropout->Linear

        # Embedding layer
        self.embedding_bag = nn.EmbeddingBag(vocab_size, embedding_dim)

        # First Linear layer
        self.linear1 = nn.Linear(embedding_dim, hidden_dim1)
        # Batch normalization for first linear layer
        self.batchnorm1 = nn.BatchNorm1d(num_features=hidden_dim1)
        # Dropout for first linear layer
        self.dropout1 = nn.Dropout(p=drop_prob1)

        # Second Linear layer
        self.linear2 = nn.Linear(hidden_dim1, hidden_dim2)
        # Batch normalization for second linear layer
        self.batchnorm2 = nn.BatchNorm1d(num_features=hidden_dim2)
        # Dropout for second linear layer
        self.dropout2 = nn.Dropout(p=drop_prob2)

        # Final Linear layer
        self.linear3 = nn.Linear(hidden_dim2, num_outputs)

    def forward(self, input_tuple):
        indices, offsets = input_tuple

        # Pass data through the embedding layer
        x = self.embedding_bag(indices, offsets)

        # First linear layer followed by ReLU, BatchNorm, and Dropout
        x = self.linear1(x)
        x = nn.ReLU()(x)
        x = self.dropout1(x)
        x = self.batchnorm1(x)

        # Second linear layer followed by ReLU, BatchNorm, and Dropout
        x = self.linear2(x)
        x = nn.ReLU()(x)
        x = self.dropout2(x)
        x = self.batchnorm2(x)

        # Final linear layer
        x = self.linear3(x)

        return x

"""#**Functions to train and evaluate the model**

##**Step Function**
"""

def step(inputs, targets, model, device, loss_function=None, optimizer=None, clip_value=10):
    """
    Performs a forward and backward pass for a given batch of inputs and targets.

    Parameters:
    - inputs (tuple): The input data for the model, as a tuple of (indices, offsets).
    - targets (torch.Tensor): The true labels for the input data.
    - model (torch.nn.Module): The neural network model.
    - device (torch.device): The computing device (CPU or GPU).
    - loss_function (torch.nn.Module, optional): The loss function to use.
    - optimizer (torch.optim.Optimizer, optional): The optimizer to update model parameters.
    - clip_value (float, optional): The value to clip the gradients to.

    Returns:
    - loss (float): The computed loss value (only if loss_function is not None).
    - outputs (torch.Tensor): The predictions from the model.
    """
    # Unpack the inputs tuple
    indices, offsets = inputs

    # Move each element of the inputs tuple and the targets to the device
    indices = indices.to(device)
    offsets = offsets.to(device)
    targets = targets.to(device)

    # Reconstruct the inputs tuple after moving to device
    inputs = (indices, offsets)

    # Forward pass to get the model's predictions
    outputs = model(inputs)

    # Compute the loss using the provided loss function
    if loss_function:
        loss = loss_function(outputs, targets)

        # Perform backward pass and update model parameters if an optimizer is provided
        if optimizer:
            optimizer.zero_grad()
            loss.backward()

            # Clip gradients before the optimizer step
            torch.nn.utils.clip_grad_value_(model.parameters(), clip_value)

            # Update parameters
            optimizer.step()

        return loss, outputs
    else:
        return None, outputs

"""##**Train Epoch**"""

def train_epoch(train_loader, model, device, loss_function, optimizer, clip_value=10):
    # Set the model to training mode
    model.train()

    # Initialize variables to track running training loss
    running_train_loss = 0.0

    # Initialize the HammingDistance metric
    train_hamming_distance = HammingDistance(task='multilabel', num_labels=10).to(device)

    # Iterate over all batches in the training data
    for inputs, targets in train_loader:
        # Perform a forward and backward pass, updating model parameters
        loss, outputs = step(inputs, targets, model, device, loss_function, optimizer, clip_value)
        running_train_loss += loss.item()

        with torch.no_grad():
            # Apply sigmoid to outputs to get the probabilities
            predictions = torch.sigmoid(outputs)
            # Update Hamming Distance
            train_hamming_distance.update((predictions > 0.5).int(), targets.int())

    # Compute average loss for the entire training set
    train_loss = running_train_loss / len(train_loader)

    # Compute Hamming Distance for the training set
    train_hamming = train_hamming_distance.compute()
    train_hamming_distance.reset()  # Reset for the next epoch

    return train_loss, train_hamming

"""##**Val Epoch**"""

def val_epoch(val_loader, model, device, loss_function):
    # Set the model to evaluation mode
    model.eval()

    # Initialize variables to track running validation loss
    running_val_loss = 0.0

    # Initialize the HammingDistance metric
    val_hamming_distance = HammingDistance(task='multilabel', num_labels=10).to(device)
    # Disable gradient computation
    with torch.no_grad():
        # Iterate over all batches in the validation data
        for inputs, targets in val_loader:
            # Perform a forward pass to get loss
            loss, outputs = step(inputs, targets, model, device, loss_function, optimizer=None)
            running_val_loss += loss.item()

            # Apply sigmoid to outputs to get the probabilities
            predictions = torch.sigmoid(outputs)
            # Update Hamming Distance
            val_hamming_distance.update((predictions > 0.5).int(), targets.int())

    # Compute average loss for the entire validation set
    val_loss = running_val_loss / len(val_loader)

    # Compute Hamming Distance for the validation set
    val_hamming = val_hamming_distance.compute()
    val_hamming_distance.reset()  # Reset for the next epoch

    return val_loss, val_hamming

"""###**Train() function**"""

def train(train_loader, val_loader, model, optimizer, loss_function, epochs, device):
    # Initialize lists to store metrics for each epoch
    train_loss_history = []
    val_loss_history = []
    train_hamming_history = []
    val_hamming_history = []

    # Loop over the number of specified epochs
    for epoch in range(epochs):
        # Train model on training data and capture metrics
        train_loss, train_hamming = train_epoch(
            train_loader, model, device, loss_function, optimizer)

        # Validate model on validation data and capture metrics
        val_loss, val_hamming = val_epoch(
            val_loader, model, device, loss_function)

        # Store metrics for this epoch
        train_loss_history.append(train_loss)
        train_hamming_history.append(train_hamming)
        val_loss_history.append(val_loss)
        val_hamming_history.append(val_hamming)

        # Output epoch-level summary
        print(f"Epoch {epoch+1}/{epochs}")
        print(f"Train Loss: {train_loss:.4f} | Train Hamming Distance: {train_hamming:.4f}")
        print(f"Valid Loss: {val_loss:.4f} | Valid Hamming Distance: {val_hamming:.4f}")
        print()

    return train_loss_history, train_hamming_history, val_loss_history, val_hamming_history

"""#**Hyperparameters and training config**"""

import torch
from torch import nn
from torch.optim import AdamW
from torchmetrics import HammingDistance

# Set the seed for reproducibility
SEED = 100
torch.manual_seed(SEED)
torch.cuda.manual_seed_all(SEED)

# Define hyperparameters
HIDDEN_DIM1 = 200
HIDDEN_DIM2 = 100
EMBED_DIM = 300
EPOCHS = 5
BATCH_SIZE = 128
LEARNING_RATE = 0.001
WEIGHT_DECAY = 0.000
CLIP_VALUE = 10
PATIENCE = 5
NUM_OUTPUTS = 10
VOCAB_SIZE = len(vocab)
DROP_PROB1 = 0.5
DROP_PROB2 = 0.5

# Determine the computing device
device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
print(f"Using device: {device}")

# Initialize the model with the specified hyperparameters
model = SimpleMLP(
    vocab_size=VOCAB_SIZE,
    embedding_dim=EMBED_DIM,
    hidden_dim1=HIDDEN_DIM1,
    hidden_dim2=HIDDEN_DIM2,
    drop_prob1=DROP_PROB1,
    drop_prob2=DROP_PROB2,
    num_outputs=NUM_OUTPUTS
)

# Transfer the model to the device
model.to(device)

# Configure the optimizer
optimizer = AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)

# Define the loss function
loss_function = nn.BCEWithLogitsLoss()

# Define collate function with a fixed vocabulary using the 'partial' function
collate_fn = partial(collate_batch, my_vocab=vocab)

# Data Loaders for training, validation, and test sets
# These loaders handle batching, shuffling, and data processing using the custom collate function
train_loader = torch.utils.data.DataLoader(train_dataset, batch_size = BATCH_SIZE, shuffle=True,
                                           collate_fn=collate_fn, num_workers=4)
valid_loader = torch.utils.data.DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False,
                                           collate_fn=collate_fn, num_workers=4)
test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False,
                                          collate_fn=collate_fn, num_workers=4)

"""##**Training**"""

# Call the training function to start the training process
train_loss_history, train_hamming_history, val_loss_history, val_hamming_history = train(
    train_loader, val_loader, model, optimizer, loss_function, EPOCHS, device
)

