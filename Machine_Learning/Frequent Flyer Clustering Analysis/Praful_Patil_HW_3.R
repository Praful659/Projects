#---------------------------------Homework 3---------------------------------
#-----------------BA With R - BUAN 6356 - SEC003 - 26942------------------------
#------------------------Praful Patil (PVP220001)-------------------------------

rm(list=ls())
cat("\014")
setwd("/Users/prafulpatil/SPRING 2023/BUAN 6356 BA(R)/Assignments/HW_3")

#1. Load the data and standardize all the variables, excluding the ID variable.
AirlineData <- read.csv("AirlineLoyalty.csv", stringsAsFactors = FALSE)
AirlineData.norm <- data.frame(sapply(AirlineData[, -1], scale))

#2. Apply hierarchical clustering with Euclidean distance and Wardâ€™s method. 
#Cut the dendrogram into two clusters. 
#Then add a column as the cluster label to the data.

###### Performing hierarchical clustering using all available variables. ######
# Compute the matrix of distances.
dist <- dist(AirlineData.norm, method = "euclidean")


Hclust <- hclust(dist(AirlineData.norm), method = "ward.D")
plot(Hclust, hang = -1, ann = FALSE)

###### Trim the dendrogram to generate k clusters.s ######
# specify # of clusters k=2
Clustered_data <- cutree(Hclust, k = 2)  # Trim the dendrogram to generate k clusters.

######## Link the cluster identifier to the original dataset "AirlineData". ########
AirlineData$h_cluster <- Clustered_data

#3. Using silhouette score to find the best k for ð‘˜ âˆˆ {1, 2,3,4,5,6,7,8,9,10}. 
#Create the silhouette plot, with x-axis being the number of clusters k, 
#y-axis being the average silhouette score .

#install.packages('cluster')  # first make sure that this is installed
library(cluster)   # for use of silhouette()

K_Table <- data.frame(matrix(ncol = 3, nrow = 0))
colnames(K_Table) <- c("numClusters", "totWithinSS", "avg_silhouette")
for (k in 1:10) {
  set.seed(4000)
  tempkm <- kmeans(AirlineData.norm,  centers = k, nstart = 10)
  
  # Note that silhouette statistics are only defined if 2 <= k <= n-1.
  if (k==1) {
    ss <- 0
  } else {
    ss <- silhouette(tempkm$cluster, dist(AirlineData.norm))[, 3]
  }
  
  # append statistics
  tempAirlineData <- data.frame(numClusters = k, totWithinSS = tempkm$tot.withinss, avg_silhouette = mean(ss))
  K_Table <- rbind(K_Table, tempAirlineData)
}

library("ggplot2")
# Silhouette
g <- ggplot(K_Table, aes(numClusters, avg_silhouette))
g <- g + geom_line() + geom_point() + labs(x="Number of Clusters (k)", y="Average Silhouette Score")
g + geom_text(aes(label=round(avg_silhouette, 3)), vjust=-0.3) + scale_x_continuous(breaks = seq(1:10))

#4. Compare the resulting clusters from step 2 and the resulting clusters when applying
#k-means with k=2. What are the characteristics of customers in each group? 
#Assume you work in the marketing department, what types of offers would you target 
#to customers in each cluster?
set.seed(4000)
Kmeans_Output <- kmeans(AirlineData.norm, centers = 2, nstart = 10)
Kmeans_Cluster <- Kmeans_Output$cluster
AirlineData$km_cluster <- Kmeans_Cluster

#comparison between Heirarchical cluster and Kmeans cluster

##A)Size
# Verify the count of data records in both K-means clustering and hierarchical clustering.
# Analyze and contrast the outcomes of the two clustering techniques.
table(Kmeans_Output$cluster) # The count of observations in each cluster of K-means clustering.
table(Clustered_data) # The count of observations in each cluster of hierarchical clustering.

#B)Visualization
###### Creating a dendrogram with a visible border around the clusters generated by hierarchical clustering. ######
plot(Hclust, hang = -1, ann = FALSE) # need to plot first before calling rect.hclust
rect.hclust(Hclust, k = 2, border = "red") # trim the dendrogram such that k clusters are produced
heatmap(as.matrix(AirlineData.norm), Colv = NA, hclustfun=function(d) hclust(d, method="ward.D"))

#####Using Factoextra to visualize our data
library(factoextra)
fviz_cluster(Kmeans_Output, geom="point", data = AirlineData.norm) + ggtitle("k=4") 


# Analyze the characteristics of customers in each group
AggregateAirlineData_hc<-aggregate(AirlineData[,c(2:ncol(AirlineData)-2)], by = list(AirlineData$h_cluster), FUN = mean); AggregateAirlineData_hc
AggregateAirlineData_km<-aggregate(AirlineData[,c(2:ncol(AirlineData)-2)], by = list(AirlineData$km_cluster), FUN = mean); AggregateAirlineData_km

#Inferences By looking at the data in the two Clusters
# So basically there are two clusters where cluster two has got data points with 
# higher numbers like Qual, bonus CC1 miles etc.. and the Awards are directly linked with these

# I would make a point that our loyal customers are present in cluster 2 and they are easy to hold on
# with less marketing but people in cluster 1 should be targeted more and marketed accordingly by
# providing offers such as Membership awards for completing certain range of miles by this way
# they continue to fly with us. 
# For People in cluster 2 who can be termed as loyal, offers should be no less than our
# competitors because we cannot lose a frequent flyer over a person in Cluster 1.
# these people in Cluster two can be offered a discount on Business class tickets.
