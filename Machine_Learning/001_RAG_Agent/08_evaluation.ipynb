{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b35536f6-166c-4b89-8136-96417db5be30",
   "metadata": {
    "id": "b35536f6-166c-4b89-8136-96417db5be30"
   },
   "source": [
    "<center><a href=\"https://www.nvidia.com/en-us/training/\"><img src=\"https://dli-lms.s3.amazonaws.com/assets/general/DLI_Header_White.png\" width=\"400\" height=\"186\" /></a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c8ac2e-eb68-4b84-85fe-3a6661eba976",
   "metadata": {
    "id": "77c8ac2e-eb68-4b84-85fe-3a6661eba976"
   },
   "source": [
    "<br>\n",
    "\n",
    "# <font color=\"#76b900\">**Notebook 8 [Assessment]:** RAG Evaluation</font>\n",
    "\n",
    "<br>\n",
    "\n",
    "Welcome to the last notebook of the course! In the previous notebook, you integrated a vector store solution into a RAG pipeline! In this notebook, you will take that same pipeline and evaluate it using numerical RAG evaluation techniques incorporating LLM-as-a-Judge metrics!\n",
    "\n",
    "<br>\n",
    "\n",
    "### **Learning Objectives:**\n",
    "\n",
    "- Learn how to integrate the techniques from prior notebooks to numerically approximate the goodness of your RAG pipeline.\n",
    "\n",
    "- **Final Exercice**: ***By working through this notebook in the Course Environment,* you will be able to submit the coding component of the course!**\n",
    "\n",
    "<br>\n",
    "\n",
    "### **Questions To Think About:**\n",
    "\n",
    "- As you go along, remember what our metrics actually represent. Should our pipeline pass these objectives? Is our judge LLM sufficient for evaluating the pipeline? Does a particular metric even matter for our use case?\n",
    "- If we left the vectorstore-as-a-memory component in our chain, do you think it would still pass the evaluation? Additionally, is the evaluation useful for assessing vectorstore-as-a-memory performance? \n",
    "\n",
    "<br>\n",
    "\n",
    "### **Notebook Source:**\n",
    "\n",
    "- This notebook is part of a larger [**NVIDIA Deep Learning Institute**](https://www.nvidia.com/en-us/training/) course titled [**Building RAG Agents with LLMs**](https://www.nvidia.com/en-sg/training/instructor-led-workshops/building-rag-agents-with-llms/). If sharing this material, please give credit and link back to the original course.\n",
    "\n",
    "<br>\n",
    "\n",
    "### **Environment Setup:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "w_A3rZOrIeQD",
   "metadata": {
    "id": "w_A3rZOrIeQD"
   },
   "outputs": [],
   "source": [
    "# %pip install -q langchain langchain-nvidia-ai-endpoints gradio rich\n",
    "# %pip install -q arxiv pymupdf faiss-cpu ragas\n",
    "\n",
    "## If you encounter a typing-extensions issue, restart your runtime and try again\n",
    "# from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "# ChatNVIDIA.get_available_models()\n",
    "\n",
    "from functools import partial\n",
    "from rich.console import Console\n",
    "from rich.style import Style\n",
    "from rich.theme import Theme\n",
    "\n",
    "console = Console()\n",
    "base_style = Style(color=\"#76B900\", bold=True)\n",
    "norm_style = Style(bold=True)\n",
    "pprint = partial(console.print, style=base_style)\n",
    "pprint2 = partial(console.print, style=norm_style)\n",
    "\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA, NVIDIAEmbeddings\n",
    "\n",
    "NVIDIAEmbeddings.get_available_models(base_url=\"http://llm_client:9000/v1\")\n",
    "embedder = NVIDIAEmbeddings(\n",
    "    model=\"nvidia/embed-qa-4\", truncate=\"END\",\n",
    "    base_url=\"http://llm_client:9000/v1\"\n",
    ")\n",
    "\n",
    "# ChatNVIDIA.get_available_models(base_url=\"http://llm_client:9000/v1\")\n",
    "instruct_llm = ChatNVIDIA(\n",
    "    model=\"mistralai/mixtral-8x22b-instruct-v0.1\",\n",
    "    base_url=\"http://llm_client:9000/v1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zEgV11oZmJGg",
   "metadata": {
    "id": "zEgV11oZmJGg"
   },
   "source": [
    "----\n",
    "\n",
    "<br>\n",
    "\n",
    "## **Part 1:** Pre-Release Evaluation\n",
    "\n",
    "In our previous notebook, we successfully combined several concepts to create a document chatbot with the aim of responsive and informative interactions. However, the diversity of user interactions necessitates comprehensive testing to truly understand the chatbot's performance. Thorough testing in varied scenarios is crucial to ensure that the system is not only robust and versatile but also aligns with user and provider expectations.\n",
    "\n",
    "After defining your chatbot's roles and implementing the necessary features, evaluating it becomes a multi-stage process:\n",
    "\n",
    "- **Typical Use Inspection:** Start by testing scenarios most relevant to your use case. See if your chatbot can reliably navigate discussions with limited human intervention.\n",
    "\n",
    "    - Additionally, identify limitations or compartments that should be redirected to a human for inspection/supervision (i.e., human swap-in to confirm transactions or perform sensitive navigation) and implement those options.\n",
    "\n",
    "- **Edge Case Inspection:** Explore the boundaries of typical use, identifying how the chatbot handles less common but plausible scenarios.\n",
    "\n",
    "    - Before any public release, assess critical boundary conditions that could pose liability risks, such as the potential generation of inappropriate content.\n",
    "\n",
    "    - Implement well-tested guardrails on all outputs (and possibly inputs) to limit undesired interactions and redirect users into predictable conversation flows.\n",
    "\n",
    "- **Progressive Rollout:** Rolling out your model to a limited audience (first internal, then [A/B](https://en.wikipedia.org/wiki/A/B_testing)) and implement analytics features like usage analytics dashboards and feedback avenues (flag/like/dislike/etc).\n",
    "\n",
    "Of these three steps, the first two can be done by a small team or an individual and should be iterated on as part of the development process. Unfortunately, this needs to be done frequently and can be prone to human error. **Luckily for us, LLMs can be used to help out with LLM-as-a-Judge formulations!**\n",
    "\n",
    "*(Yeah, this probably isn't surprising by now. LLMs being strong is why this course is here...).*\n",
    "\n",
    "----\n",
    "\n",
    "<br>\n",
    "\n",
    "## **Part 2:** LLM-as-a-Judge Formulation\n",
    "\n",
    "In the realm of conversational AI, using LLMs as evaluators or 'judges' has emerged as a useful approach for configurable automatic testing of natural language task performance:\n",
    "\n",
    "- An LLM can simulate a range of interaction scenarios and generate synthetic data, allowing an evaluation developer to generate targeted inputs to eliciting a range of behaviors from your chatbot.\n",
    "\n",
    "- The chatbot's correspondence/retrieval on the synthetic data can be evaluated or parsed by an LLM and a consistent output format such as \"Pass\"/\"Fail\", similarity, or extraction can be enforced.\n",
    "\n",
    "- Many such results can be aggregated and a metric can be derived which explains something like \"% of passing evaluations\", \"average number of relevant details from the sources\", \"average cosine similarity\", etc.\n",
    "\n",
    "This idea of using LLMs to test out and quantify chatbot quality, known as [**\"LLM-as-a-Judge,\"**](https://arxiv.org/abs/2306.05685) allows for easy test specifications that align closely with human judgment and can be fine-tuned and replicated at scale.\n",
    "\n",
    "**There are several popular frameworks for off-the-shelf judge formulations including:**\n",
    "- [**RAGAs (short for RAG Assessment)**](https://docs.ragas.io/en/stable/), which offers a suite of great starting points for your own evaluation efforts.\n",
    "- [**LangChain Evaluators**](https://python.langchain.com/docs/guides/evaluation/), which are similar first-party options with many implicitly-constructible agents.\n",
    "\n",
    "Instead of using the chains as-is, we will instead expand on the ideas and evaluate our system with a more custom solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fDDNaBA9N3XM",
   "metadata": {
    "id": "fDDNaBA9N3XM"
   },
   "source": [
    "----\n",
    "\n",
    "<br>\n",
    "\n",
    "## **Part 3: [Assessment Prep]** Pairwise Evaluator\n",
    "\n",
    "The following exercise will flesh out a custom implementation of a simplified [LangChain Pairwise String Evaluator](https://python.langchain.com/docs/guides/evaluation/examples/comparisons). \n",
    "\n",
    "**To prepare for our RAG chain evaluation, we will need to:**\n",
    "\n",
    "- Pull in our document index (the one we saved in the previous notebook).\n",
    "- Recreate our RAG pipeline of choice.\n",
    "\n",
    "**We will specifically be implementing a judge formulation with the following steps:**\n",
    "\n",
    "- Sample the RAG agent document pool to find two document chunks.\n",
    "- Use those two document chunks to generate a synthetic \"baseline\" question-answer pair.\n",
    "- Use the RAG agent to generate its own answer.\n",
    "- Use a judge LLM to compare the two responses while grounding the synthetic generation as \"ground-truth correct.\"\n",
    "\n",
    "**The chain should be a simple but powerful process that tests for the following objective:**\n",
    "\n",
    "> ***Does my RAG chain outperform a narrow chatbot with limited document access.***\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**This will be the system used for the final evaluation!** To see how this system is integrated into the autograder, please check out the implementation in [`frontend/server_app.py`](frontend/server_app.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bh8jaOqak0f",
   "metadata": {
    "id": "1bh8jaOqak0f"
   },
   "source": [
    "<br>\n",
    "\n",
    "### **Task 1:** Pull In Your Document Retrieval Index\n",
    "\n",
    "For this exercise, you will pull in the `docstore_index` file you created as part of your earlier notebook. The following cell should be able to load in the store as-is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "tlE7a2lseLOy",
   "metadata": {
    "id": "tlE7a2lseLOy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docstore_index/\n",
      "docstore_index/index.pkl\n",
      "docstore_index/index.faiss\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Constructed aggregate docstore with </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">238</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> chunks</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mConstructed aggregate docstore with \u001b[0m\u001b[1;36m238\u001b[0m\u001b[1;38;2;118;185;0m chunks\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Sample Chunk:</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mSample Chunk:\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paper: Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\n",
      "\n",
      "Summary: Large pre-trained language models have been shown to store factual knowledge\n",
      "in their parameters, and achieve state-of-the-art results when fine-tuned on\n",
      "downstream NLP tasks. However, their ability to access and precisely manipulate\n",
      "knowledge is still limited, and hence on knowledge-intensive tasks, their\n",
      "performance lags behind task-specific architectures. Additionally, providing\n",
      "provenance for their decisions and updating their world knowledge remain open\n",
      "research problems. Pre-trained models with a differentiable access mechanism to\n",
      "explicit non-parametric memory can overcome this issue, but have so far been\n",
      "only investigated for extractive downstream tasks. We explore a general-purpose\n",
      "fine-tuning recipe for retrieval-augmented generation (RAG) -- models which\n",
      "combine pre-trained parametric and non-parametric memory for language\n",
      "generation. We introduce RAG models where the parametric memory is a\n",
      "pre-trained seq2seq model and the non-parametric memory is a dense vector index\n",
      "of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG\n",
      "formulations, one which conditions on the same retrieved passages across the\n",
      "whole generated sequence, the other can use different passages per token. We\n",
      "fine-tune and evaluate our models on a wide range of knowledge-intensive NLP\n",
      "tasks and set the state-of-the-art on three open domain QA tasks, outperforming\n",
      "parametric seq2seq models and task-specific retrieve-and-extract architectures.\n",
      "For language generation tasks, we find that RAG models generate more specific,\n",
      "diverse and factual language than a state-of-the-art parametric-only seq2seq\n",
      "baseline.\n",
      "\n",
      "Page Body: .S.\\nRAG-T It\\u2019s the only U.S. state named for a U.S. president\\nRAG-S It\\u2019s the state where you\\u2019ll \\ufb01nd Mount Rainier National Park\\nThe Divine\\nComedy\\nBART\\n*This epic poem by Dante is divided into 3 parts: the Inferno, the Purgatorio & the Purgatorio\\nRAG-T Dante\\u2019s \\\"Inferno\\\" is the \\ufb01rst part of this epic poem\\nRAG-S This 14th century work is divided into 3 sections: \\\"Inferno\\\", \\\"Purgatorio\\\" & \\\"Paradiso\\\"\\nFor 2-way classi\\ufb01cation, we compare against Thorne and Vlachos [57], who train RoBERTa [35]\\nto classify the claim as true or false given the gold evidence sentence. RAG achieves an accuracy\\nwithin 2.7% of this model, despite being supplied with only the claim and retrieving its own evidence.\\nWe also analyze whether documents retrieved by RAG correspond to documents annotated as gold\\nevidence in FEVER. We calculate the overlap in article titles between the top k documents retrieved\\nby RAG and gold evidence annotations\n"
     ]
    }
   ],
   "source": [
    "## Make sure you have docstore_index.tgz in your working directory\n",
    "from langchain_nvidia_ai_endpoints import NVIDIAEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# embedder = NVIDIAEmbeddings(model=\"nvidia/embed-qa-4\", truncate=\"END\")\n",
    "\n",
    "!tar xzvf docstore_index.tgz\n",
    "docstore = FAISS.load_local(\"docstore_index\", embedder, allow_dangerous_deserialization=True)\n",
    "docs = list(docstore.docstore._dict.values())\n",
    "\n",
    "def format_chunk(doc):\n",
    "    return (\n",
    "        f\"Paper: {doc.metadata.get('Title', 'unknown')}\"\n",
    "        f\"\\n\\nSummary: {doc.metadata.get('Summary', 'unknown')}\"\n",
    "        f\"\\n\\nPage Body: {doc.page_content}\"\n",
    "    )\n",
    "\n",
    "## This printout just confirms that your store has been retrieved\n",
    "pprint(f\"Constructed aggregate docstore with {len(docstore.docstore._dict)} chunks\")\n",
    "pprint(f\"Sample Chunk:\")\n",
    "print(format_chunk(docs[len(docs)//2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dib0F-t2N4LJ",
   "metadata": {
    "id": "dib0F-t2N4LJ"
   },
   "source": [
    "<br>\n",
    "\n",
    "### **Task 2: [Exercise]** Pull In Your RAG Chain\n",
    "\n",
    "Now that we have our index, we can recreate the RAG agent from the previous notebook! \n",
    "\n",
    "**Key Modifications:**\n",
    "- To keep things simple, feel free to disregard the vectorstore-as-a-memory component. Incorporating it will require some more overhead and will make the exercise a bit more complicated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "XBi6Y8b8aXd2",
   "metadata": {
    "id": "XBi6Y8b8aXd2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': 'Tell me something interesting!', 'context': '[Quote from Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena] .\\\\nThis paper makes two contributions: (1) a systematic study of LLM-as-a-judge; and (2) human\\\\npreference datasets with high-quality questions and diverse user interactions from MT-bench and\\\\nChatbot Arena. In addition, we argue for the adoption of a hybrid evaluation framework for future\\\\nLLM benchmarks: by combining the existing capability-based benchmarks and the new preference-\\\\nbased benchmarks with LLM-as-a-judge, one can swiftly and automatically evaluate both the core\\\\ncapabilities and human alignment of models. We publicly release 80 MT-bench questions, 3K expert\\\\nvotes, and 30K conversations with human preferences for future study.\\\\nTable 1: Sample multi-turn questions in MT-bench.\\\\nCategory\\\\nSample Questions\\\\nWriting\\\\n1st Turn\\\\nCompose an engaging travel blog post about a recent trip to Hawaii, highlighting\\\\ncultural experiences and must-see attractions.\\\\n2nd Turn\\\\nRewrite your previous response. Start every sentence with the letter A\\n[Quote from Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks] . REALM [20] and ORQA [31], two recently introduced models that\\\\ncombine masked language models [8] with a differentiable retriever, have shown promising results,\\\\narXiv:2005.11401v4  [cs.CL]  12 Apr 2021\\\\nThe Divine\\\\nComedy (x)\\\\nq\\\\nQuery\\\\nEncoder\\\\nq(x)\\\\nMIPS\\\\np\\\\u03b8\\\\nGenerator\\\\u00a0p\\\\u03b8\\\\n(Parametric)\\\\nMargin-\\\\nalize\\\\nThis 14th century work\\\\nis divided into 3\\\\nsections: \\\\\"Inferno\\\\\",\\\\n\\\\\"Purgatorio\\\\\" &\\\\n\\\\\"Paradiso\\\\\"         (y)\\\\nEnd-to-End Backprop through q and\\\\u00a0p\\\\u03b8\\\\nBarack Obama was\\\\nborn in Hawaii.(x)\\\\nFact Veri\\\\ufb01cation: Fact Query\\\\nsupports (y)\\\\nQuestion Generation\\\\nFact Veri\\\\ufb01cation:\\\\nLabel Generation\\\\nDocument\\\\nIndex\\\\nDefine \\\\\"middle ear\\\\\"(x)\\\\nQuestion Answering:\\\\nQuestion Query\\\\nThe middle ear includes\\\\nthe tympanic cavity and\\\\nthe three ossicles.  (y)\\\\nQuestion Answering:\\\\nAnswer Generation\\\\nRetriever p\\\\u03b7\\\\n(Non-Parametric)\\\\nz4\\\\nz3\\\\nz2\\\\nz1\\\\nd(z)\\\\nJeopardy Question\\\\nGeneration:\\\\nAnswer Query\\\\nFigure 1: Overview of our approach\\n[Quote from Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks] .\\\\n9\\\\nBroader Impact\\\\nThis work offers several positive societal bene\\\\ufb01ts over previous work: the fact that it is more\\\\nstrongly grounded in real factual knowledge (in this case Wikipedia) makes it \\\\u201challucinate\\\\u201d less\\\\nwith generations that are more factual, and offers more control and interpretability. RAG could be\\\\nemployed in a wide variety of scenarios with direct bene\\\\ufb01t to society, for example by endowing it\\\\nwith a medical index and asking it open-domain questions on that topic, or by helping people be more\\\\neffective at their jobs.\\\\nWith these advantages also come potential downsides: Wikipedia, or any potential external knowledge\\\\nsource, will probably never be entirely factual and completely devoid of bias\\n[Quote from Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks] . Rather than use questions from standard open-domain QA tasks, which typically consist\\\\nof short, simple questions, we propose the more demanding task of generating Jeopardy questions.\\\\nJeopardy is an unusual format that consists of trying to guess an entity from a fact about that entity.\\\\nFor example, \\\\u201cThe World Cup\\\\u201d is the answer to the question \\\\u201cIn 1986 Mexico scored as the \\\\ufb01rst\\\\ncountry to host this international sports competition twice.\\\\u201d As Jeopardy questions are precise,\\\\nfactual statements, generating Jeopardy questions conditioned on their answer entities constitutes a\\\\nchallenging knowledge-intensive generation task.\\\\nWe use the splits from SearchQA [10], with 100K train, 14K dev, and 27K test examples. As\\\\nthis is a new task, we train a BART model for comparison. Following [67], we evaluate using the\\\\nSQuAD-tuned Q-BLEU-1 metric [42]\\n', 'output': 'The document discusses a paper that introduces a systematic study of LLM-as-a-judge and offers human preference datasets from MT-bench and Chatbot Arena. The authors argue for the adoption of a hybrid evaluation framework for future LLM benchmarks. Interestingly, they mention that by combining existing capability-based benchmarks and new preference-based benchmarks with LLM-as-a-judge, one can swiftly and automatically evaluate both the core capabilities and human alignment of models. They publicly release 80 MT-bench questions, 3K expert votes, and 30K conversations with human preferences for future study.\\n\\nAnother interesting aspect mentioned in the document is a model that combines masked language models with a differentiable retriever (e.g., REALM and ORQA), which has shown promising results for knowledge-intensive NLP tasks. This model uses an end-to-end backpropagation approach and offers several positive societal benefits, such as hallucinating less with more factual generations, providing control, and being interpretable. However, potential downsides include the fact that sources like Wikipedia may never be entirely factual and devoid of bias.\\n\\nLastly, when discussing tasks for this model, the authors propose generating Jeopardy questions instead of using short and simple questions from standard open-domain QA tasks. Generating Jeopardy questions from factual entities poses a challenging knowledge-intensive generation task.\\n\\n[Quote from Judging LLM-as-a-Judge with MT-bench and Chatbot Arena]\\n[Quote from Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks]'}"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda, RunnableBranch\n",
    "from langchain_core.runnables.passthrough import RunnableAssign\n",
    "from langchain.document_transformers import LongContextReorder\n",
    "from langserve import add_routes\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA, NVIDIAEmbeddings\n",
    "from fastapi import FastAPI\n",
    "from functools import partial\n",
    "from operator import itemgetter\n",
    "\n",
    "import gradio as gr\n",
    "\n",
    "#####################################################################\n",
    "\n",
    "# NVIDIAEmbeddings.get_available_models(base_url=\"http://llm_client:9000/v1\")\n",
    "embedder = NVIDIAEmbeddings(\n",
    "    model=\"nvidia/embed-qa-4\", truncate=\"END\",\n",
    "    base_url=\"http://llm_client:9000/v1\"\n",
    ")\n",
    "\n",
    "# ChatNVIDIA.get_available_models(base_url=\"http://llm_client:9000/v1\")\n",
    "instruct_llm = ChatNVIDIA(\n",
    "    model=\"mistralai/mixtral-8x22b-instruct-v0.1\",\n",
    "    base_url=\"http://llm_client:9000/v1\"\n",
    ")\n",
    "llm = instruct_llm | StrOutputParser()\n",
    "\n",
    "#####################################################################\n",
    "\n",
    "def docs2str(docs, title=\"Document\"):\n",
    "    \"\"\"Useful utility for making chunks into context string. Optional, but useful\"\"\"\n",
    "    out_str = \"\"\n",
    "    for doc in docs:\n",
    "        doc_name = getattr(doc, 'metadata', {}).get('Title', title)\n",
    "        if doc_name: out_str += f\"[Quote from {doc_name}] \"\n",
    "        out_str += getattr(doc, 'page_content', str(doc)) + \"\\n\"\n",
    "    return out_str\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_template(\n",
    "    \"You are a document chatbot. Help the user as they ask questions about documents.\"\n",
    "    \" User messaged just asked you a question: {input}\\n\\n\"\n",
    "    \" The following information may be useful for your response: \"\n",
    "    \" Document Retrieval:\\n{context}\\n\\n\"\n",
    "    \" (Answer only from retrieval. Only cite sources that are used. Make your response conversational)\"\n",
    "    \"\\n\\nUser Question: {input}\"\n",
    ")\n",
    "\n",
    "def output_puller(inputs):\n",
    "    \"\"\"\"Output generator. Useful if your chain returns a dictionary with key 'output'\"\"\"\n",
    "    if isinstance(inputs, dict):\n",
    "        inputs = [inputs]\n",
    "    for token in inputs:\n",
    "        if token.get('output'):\n",
    "            yield token.get('output')\n",
    "\n",
    "#####################################################################\n",
    "## TODO: Pull in your desired RAG Chain. Memory not necessary\n",
    "\n",
    "## Chain 1 Specs: \"Hello World\" -> retrieval_chain \n",
    "##   -> {'input': <str>, 'context' : <str>}\n",
    "long_reorder = RunnableLambda(LongContextReorder().transform_documents)  ## GIVEN\n",
    "context_getter = itemgetter('input') | docstore.as_retriever() | long_reorder | docs2str  ## TODO\n",
    "retrieval_chain = {'input' : (lambda x: x)} | RunnableAssign({'context' : context_getter})\n",
    "\n",
    "## Chain 2 Specs: retrieval_chain -> generator_chain \n",
    "##   -> {\"output\" : <str>, ...} -> output_puller\n",
    "generator_chain = RunnableAssign({\"output\" : chat_prompt | llm })  ## TODO\n",
    "generator_chain = {'output' : generator_chain} | RunnableLambda(output_puller)  ## GIVEN\n",
    "\n",
    "## END TODO\n",
    "#####################################################################\n",
    "\n",
    "rag_chain = retrieval_chain | generator_chain\n",
    "\n",
    "app = FastAPI(\n",
    "  title=\"LangChain Server\",\n",
    "  version=\"1.0\",\n",
    "  description=\"A simple api server using Langchain's Runnable interfaces\",\n",
    ")\n",
    "\n",
    "add_routes(\n",
    "    app,\n",
    "    rag_chain,\n",
    "    path=\"/generator\",\n",
    ")\n",
    "\n",
    "# pprint(rag_chain.invoke(\"Tell me something interesting!\"))\n",
    "for token in rag_chain.stream(\"Tell me something interesting!\"):\n",
    "    print(token, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b880971-d3a0-433f-a60b-e8a4edb754c8",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### **Step 3:** Generating Synthetic Question-Answer Pairs\n",
    "\n",
    "In this section, we can implement the first few part of our evaluation routine:\n",
    "\n",
    "- **Sample the RAG agent document pool to find two document chunks.**\n",
    "- **Use those two document chunks to generate a synthetic \"baseline\" question-answer pair.**\n",
    "- Use the RAG agent to generate its own answer.\n",
    "- Use a judge LLM to compare the two responses while grounding the synthetic generation as \"ground-truth correct.\"\n",
    "\n",
    "The chain should be a simple but powerful process that tests for the following objective:\n",
    "\n",
    "> Does my RAG chain outperform a narrow chatbot with limited document access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ymzuX-DSNvL6",
   "metadata": {
    "id": "ymzuX-DSNvL6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">QA Pair </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mQA Pair \u001b[0m\u001b[1;36m1\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Question: How does Mistral 7B improve performance and efficiency compared to other language models, such as Llama </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>\n",
       "<span style=\"font-weight: bold\">13B and Llama </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\"> 34B? Additionally, how can large language model (LLM) based chat assistants be effectively </span>\n",
       "<span style=\"font-weight: bold\">evaluated using strong LLMs as judges?</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mQuestion: How does Mistral 7B improve performance and efficiency compared to other language models, such as Llama \u001b[0m\u001b[1;36m2\u001b[0m\n",
       "\u001b[1m13B and Llama \u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m 34B? Additionally, how can large language model \u001b[0m\u001b[1m(\u001b[0m\u001b[1mLLM\u001b[0m\u001b[1m)\u001b[0m\u001b[1m based chat assistants be effectively \u001b[0m\n",
       "\u001b[1mevaluated using strong LLMs as judges?\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Answer: Mistral 7B outperforms Llama </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> 13B across all evaluated benchmarks and Llama </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> 34B in reasoning, </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">mathematics, and code generation. It leverages grouped-query attention (GQA) for faster inference and sliding </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">window attention (SWA) to handle sequences of arbitrary length with reduced inference cost. The model's </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">architecture includes a transformer with the following parameters: dim (</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4096</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">), n_layers (</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">32</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">), head_dim (</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">128</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">), </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">hidden_dim (</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">14336</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">), n_heads (</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">32</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">), n_kv_heads (</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">), window_size (</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4096</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">), context_len (</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8192</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">), and vocab_size (</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">32000</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">).</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mAnswer: Mistral 7B outperforms Llama \u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;38;2;118;185;0m 13B across all evaluated benchmarks and Llama \u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;38;2;118;185;0m 34B in reasoning, \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mmathematics, and code generation. It leverages grouped-query attention \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mGQA\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m for faster inference and sliding \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mwindow attention \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mSWA\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m to handle sequences of arbitrary length with reduced inference cost. The model's \u001b[0m\n",
       "\u001b[1;38;2;118;185;0marchitecture includes a transformer with the following parameters: dim \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;36m4096\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m, n_layers \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m, head_dim \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m, \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mhidden_dim \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;36m14336\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m, n_heads \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m, n_kv_heads \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m, window_size \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;36m4096\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m, context_len \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;36m8192\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m, and vocab_size \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;36m32000\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">QA Pair </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mQA Pair \u001b[0m\u001b[1;36m2\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Question: How does the proposed Transformer model in </span><span style=\"color: #008000; text-decoration-color: #008000\">\"Attention Is All You Need\"</span><span style=\"font-weight: bold\"> differ from the existing sequence </span>\n",
       "<span style=\"font-weight: bold\">transduction models, and what improvements does it bring to machine translation tasks? How well does it generalize </span>\n",
       "<span style=\"font-weight: bold\">to other tasks like English constituency parsing?</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mQuestion: How does the proposed Transformer model in \u001b[0m\u001b[32m\"Attention Is All You Need\"\u001b[0m\u001b[1m differ from the existing sequence \u001b[0m\n",
       "\u001b[1mtransduction models, and what improvements does it bring to machine translation tasks? How well does it generalize \u001b[0m\n",
       "\u001b[1mto other tasks like English constituency parsing?\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Answer: The proposed Transformer model is a new simple network architecture based solely on attention mechanisms, </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">dispensing with recurrence and convolutions entirely. Unlike the dominant sequence transduction models which are </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">based on complex recurrent or convolutional neural networks in an encoder-decoder configuration, the Transformer </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">model connects the encoder and decoder through an attention mechanism. Experiments on two machine translation </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">tasks, WMT </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2014</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> English-to-German and English-to-French translation tasks, show that the Transformer models are </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">superior in quality while being more parallelizable and requiring significantly less time to train. The Transformer</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">model achieves </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">28.4</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> BLEU on the WMT </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2014</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> English-to-German translation task, improving over the existing best </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">results by over </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> BLEU, and establishes a new single-model state-of-the-art BLEU score of </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">41.8</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> after training for </span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3.5</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> days on eight GPUs for the English-to-French translation task. The Transformer generalizes well to other tasks </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">such as English constituency parsing with large and limited training data.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mAnswer: The proposed Transformer model is a new simple network architecture based solely on attention mechanisms, \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mdispensing with recurrence and convolutions entirely. Unlike the dominant sequence transduction models which are \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mbased on complex recurrent or convolutional neural networks in an encoder-decoder configuration, the Transformer \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mmodel connects the encoder and decoder through an attention mechanism. Experiments on two machine translation \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mtasks, WMT \u001b[0m\u001b[1;36m2014\u001b[0m\u001b[1;38;2;118;185;0m English-to-German and English-to-French translation tasks, show that the Transformer models are \u001b[0m\n",
       "\u001b[1;38;2;118;185;0msuperior in quality while being more parallelizable and requiring significantly less time to train. The Transformer\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mmodel achieves \u001b[0m\u001b[1;36m28.4\u001b[0m\u001b[1;38;2;118;185;0m BLEU on the WMT \u001b[0m\u001b[1;36m2014\u001b[0m\u001b[1;38;2;118;185;0m English-to-German translation task, improving over the existing best \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mresults by over \u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;38;2;118;185;0m BLEU, and establishes a new single-model state-of-the-art BLEU score of \u001b[0m\u001b[1;36m41.8\u001b[0m\u001b[1;38;2;118;185;0m after training for \u001b[0m\n",
       "\u001b[1;36m3.5\u001b[0m\u001b[1;38;2;118;185;0m days on eight GPUs for the English-to-French translation task. The Transformer generalizes well to other tasks \u001b[0m\n",
       "\u001b[1;38;2;118;185;0msuch as English constituency parsing with large and limited training data.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">QA Pair </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mQA Pair \u001b[0m\u001b[1;36m3\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Question: What are the inherent limitations of current Large Language Models (LMs) and how does the proposed </span>\n",
       "<span style=\"font-weight: bold\">neuro-symbolic architecture, the Modular Reasoning, Knowledge and Language (MRKL) system, aim to address these </span>\n",
       "<span style=\"font-weight: bold\">limitations? How does this system compare in performance to other sequence transduction models like the Transformer</span>\n",
       "<span style=\"font-weight: bold\">model?</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mQuestion: What are the inherent limitations of current Large Language Models \u001b[0m\u001b[1m(\u001b[0m\u001b[1mLMs\u001b[0m\u001b[1m)\u001b[0m\u001b[1m and how does the proposed \u001b[0m\n",
       "\u001b[1mneuro-symbolic architecture, the Modular Reasoning, Knowledge and Language \u001b[0m\u001b[1m(\u001b[0m\u001b[1mMRKL\u001b[0m\u001b[1m)\u001b[0m\u001b[1m system, aim to address these \u001b[0m\n",
       "\u001b[1mlimitations? How does this system compare in performance to other sequence transduction models like the Transformer\u001b[0m\n",
       "\u001b[1mmodel?\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Answer: The inherent limitations of current Large Language Models (LMs) are that despite their versatility and </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">impressive capabilities, their output is often wrong or ridiculously incorrect. This is exemplified by the sample </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">output of GPT-</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> on simple queries. The proposed neuro-symbolic architecture, the Modular Reasoning, Knowledge and </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Language (MRKL) system, aims to address these limitations by adopting a systems approach that involves knowledge </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">and reasoning in addition to linguistic processing. The MRKL system is a flexible architecture with multiple neural</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">models, complemented by discrete knowledge and reasoning modules. In terms of performance, the Transformer model, a</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">sequence transduction model based solely on attention mechanisms, has been shown to be superior in quality while </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">being more parallelizable and requiring significantly less time to train. However, the MRKL system's performance is</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">not compared to the Transformer model in the provided document.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mAnswer: The inherent limitations of current Large Language Models \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mLMs\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m are that despite their versatility and \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mimpressive capabilities, their output is often wrong or ridiculously incorrect. This is exemplified by the sample \u001b[0m\n",
       "\u001b[1;38;2;118;185;0moutput of GPT-\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;38;2;118;185;0m on simple queries. The proposed neuro-symbolic architecture, the Modular Reasoning, Knowledge and \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mLanguage \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mMRKL\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m system, aims to address these limitations by adopting a systems approach that involves knowledge \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mand reasoning in addition to linguistic processing. The MRKL system is a flexible architecture with multiple neural\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mmodels, complemented by discrete knowledge and reasoning modules. In terms of performance, the Transformer model, a\u001b[0m\n",
       "\u001b[1;38;2;118;185;0msequence transduction model based solely on attention mechanisms, has been shown to be superior in quality while \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mbeing more parallelizable and requiring significantly less time to train. However, the MRKL system's performance is\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mnot compared to the Transformer model in the provided document.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "num_questions = 3\n",
    "synth_questions = []\n",
    "synth_answers = []\n",
    "\n",
    "simple_prompt = ChatPromptTemplate.from_template('INSTRUCTION:\\n\\n{system}\\n\\nINPUT:\\n\\n{input}')\n",
    "\n",
    "for i in range(num_questions):\n",
    "    doc1, doc2 = random.sample(docs, 2)\n",
    "    sys_msg = (\n",
    "        \"Use the documents provided by the user to generate an interesting question-answer pair.\"\n",
    "        \" Try to use both documents if possible, and rely more on the document bodies than the summary.\"\n",
    "        \" Use the format:\\nQuestion: (good question, 1-3 sentences, detailed)\\n\\nAnswer: (answer derived from the documents)\"\n",
    "    )\n",
    "    usr_msg = (\n",
    "        f\"Document1: {format_chunk(doc1)}\\n\\n\"\n",
    "        f\"Document2: {format_chunk(doc2)}\"\n",
    "    )\n",
    "\n",
    "    qa_pair = (simple_prompt | llm).invoke({'system': sys_msg, 'input': usr_msg})\n",
    "    synth_questions += [qa_pair.split('\\n\\n')[0]]\n",
    "    synth_answers += [qa_pair.split('\\n\\n')[1]]\n",
    "    pprint2(f\"QA Pair {i+1}\")\n",
    "    pprint2(synth_questions[-1])\n",
    "    pprint(synth_answers[-1])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5Q-3X4vS98P",
   "metadata": {
    "id": "c5Q-3X4vS98P"
   },
   "source": [
    "<br>\n",
    "\n",
    "### **Step 4:** Answer The Synthetic Questions\n",
    "\n",
    "In this section, we can implement the third part of our evaluation routine:\n",
    "\n",
    "- Sample the RAG agent document pool to find two document chunks.\n",
    "- Use those two document chunks to generate a synthetic \"baseline\" question-answer pair.\n",
    "- **Use the RAG agent to generate its own answer.**\n",
    "- Use a judge LLM to compare the two responses while grounding the synthetic generation as \"ground-truth correct.\"\n",
    "\n",
    "The chain should be a simple but powerful process that tests for the following objective:\n",
    "\n",
    "> Does my RAG chain outperform a narrow chatbot with limited document access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7T3GSwhZPHjF",
   "metadata": {
    "id": "7T3GSwhZPHjF"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">QA Pair </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>\n",
       "<span style=\"font-weight: bold\">Question: How does Mistral 7B improve performance and efficiency compared to other language models, such as Llama </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>\n",
       "<span style=\"font-weight: bold\">13B and Llama </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\"> 34B? Additionally, how can large language model (LLM) based chat assistants be effectively </span>\n",
       "<span style=\"font-weight: bold\">evaluated using strong LLMs as judges?</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mQA Pair \u001b[0m\u001b[1;36m1\u001b[0m\n",
       "\u001b[1mQuestion: How does Mistral 7B improve performance and efficiency compared to other language models, such as Llama \u001b[0m\u001b[1;36m2\u001b[0m\n",
       "\u001b[1m13B and Llama \u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m 34B? Additionally, how can large language model \u001b[0m\u001b[1m(\u001b[0m\u001b[1mLLM\u001b[0m\u001b[1m)\u001b[0m\u001b[1m based chat assistants be effectively \u001b[0m\n",
       "\u001b[1mevaluated using strong LLMs as judges?\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">RAG Answer: Based on the provided document, Mistral 7B improves performance and efficiency compared to other </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">language models such as Llama </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> 13B and Llama </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> 34B in several ways.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Firstly, Mistral 7B is a language model with </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> billion parameters that is engineered for superior performance and </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">efficiency. It outperforms the best open 13B model (Llama </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">) across all evaluated benchmarks and the best released </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">34B model (Llama </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">) in reasoning, mathematics, and code generation (Source: Mistral 7B).</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Secondly, Mistral 7B leverages grouped-query attention (GQA) for faster inference, and sliding window attention </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(SWA) to effectively handle sequences of arbitrary length with a reduced inference cost. This makes it more </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">efficient than other models (Source: Mistral 7B).</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">As for evaluating large language model (LLM) based chat assistants, it can be challenging due to their broad </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">capabilities and the inadequacy of existing benchmarks in measuring human preferences. However, the usage of strong</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">LLMs as judges to evaluate these models on more open-ended questions has been explored. This method, known as </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">LLM-as-a-judge, examines the usage and limitations of this approach, including position, verbosity, and </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">self-enhancement biases, as well as limited reasoning ability. Solutions to mitigate some of these issues have also</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">been proposed (Source: Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena).</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mRAG Answer: Based on the provided document, Mistral 7B improves performance and efficiency compared to other \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mlanguage models such as Llama \u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;38;2;118;185;0m 13B and Llama \u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;38;2;118;185;0m 34B in several ways.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mFirstly, Mistral 7B is a language model with \u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;38;2;118;185;0m billion parameters that is engineered for superior performance and \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mefficiency. It outperforms the best open 13B model \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mLlama \u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m across all evaluated benchmarks and the best released \u001b[0m\n",
       "\u001b[1;38;2;118;185;0m34B model \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mLlama \u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m in reasoning, mathematics, and code generation \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mSource: Mistral 7B\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mSecondly, Mistral 7B leverages grouped-query attention \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mGQA\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m for faster inference, and sliding window attention \u001b[0m\n",
       "\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mSWA\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m to effectively handle sequences of arbitrary length with a reduced inference cost. This makes it more \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mefficient than other models \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mSource: Mistral 7B\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mAs for evaluating large language model \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mLLM\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m based chat assistants, it can be challenging due to their broad \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mcapabilities and the inadequacy of existing benchmarks in measuring human preferences. However, the usage of strong\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mLLMs as judges to evaluate these models on more open-ended questions has been explored. This method, known as \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mLLM-as-a-judge, examines the usage and limitations of this approach, including position, verbosity, and \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mself-enhancement biases, as well as limited reasoning ability. Solutions to mitigate some of these issues have also\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mbeen proposed \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mSource: Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m.\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">QA Pair </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>\n",
       "<span style=\"font-weight: bold\">Question: How does the proposed Transformer model in </span><span style=\"color: #008000; text-decoration-color: #008000\">\"Attention Is All You Need\"</span><span style=\"font-weight: bold\"> differ from the existing sequence </span>\n",
       "<span style=\"font-weight: bold\">transduction models, and what improvements does it bring to machine translation tasks? How well does it generalize </span>\n",
       "<span style=\"font-weight: bold\">to other tasks like English constituency parsing?</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mQA Pair \u001b[0m\u001b[1;36m2\u001b[0m\n",
       "\u001b[1mQuestion: How does the proposed Transformer model in \u001b[0m\u001b[32m\"Attention Is All You Need\"\u001b[0m\u001b[1m differ from the existing sequence \u001b[0m\n",
       "\u001b[1mtransduction models, and what improvements does it bring to machine translation tasks? How well does it generalize \u001b[0m\n",
       "\u001b[1mto other tasks like English constituency parsing?\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">RAG Answer: The Transformer model proposed in </span><span style=\"color: #008000; text-decoration-color: #008000\">\"Attention Is All You Need\"</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> differs from existing sequence </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">transduction models because it is the first transduction model that relies entirely on self-attention to compute </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">representations of its input and output, without using sequence-aligned RNNs (Recurrent Neural Networks) or </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">convolution. Most competitive neural sequence transduction models have an encoder-decoder structure, where the </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">encoder maps an input sequence of symbol representations to a sequence of continuous representations, and then the </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">decoder generates an output sequence of symbols one element at a time.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">The Transformer model replaces the recurrent layers most commonly used in encoder-decoder architectures with </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">multi-headed self-attention. This architecture allows the model to be trained significantly faster than </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">architectures based on recurrent or convolutional layers. The model has been applied to machine translation tasks </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">and has achieved a new state of the art on both WMT </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2014</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> English-to-German and WMT </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2014</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> English-to-French </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">translation tasks. In the former task, the best model outperforms even all previously reported ensembles.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">The Transformer model also generalizes well to other tasks, such as English constituency parsing. The authors show </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">that the model can be successfully applied to English constituency parsing both with large and limited training </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">data. The model's performance on these tasks suggests that attention-based models have the potential to be applied </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">to a wide range of tasks beyond machine translation.</span>\n",
       "\n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Reference</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(s):</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">[Quote from Attention Is All You Need] . (</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2023</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">). Attention Is All You Need. Retrieved from </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">&lt;</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://arxiv.org/pdf/1706.03762.pdf</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">&gt;</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">Summary:</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">In </span><span style=\"color: #008000; text-decoration-color: #008000\">\"Attention Is All You Need,\"</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\"> the authors propose the Transformer model, a new simple network architecture based </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">solely on attention mechanisms, dispensing with recurrence and convolutions entirely. The model has been shown to </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">be superior in quality while being more parallelizable and requiring significantly less time to train. It has </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">achieved a new state of the art on machine translation tasks and has been successfully applied to English </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">constituency parsing with both large and limited training data.</span>\n",
       "\n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Source</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(s):</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\"Attention Is All You Need\"</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\"> (</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2023</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">), A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">Kaiser, and I. Polosukhin, &lt;</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://arxiv.org/pdf/1706.03762.pdf</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">&gt;</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mRAG Answer: The Transformer model proposed in \u001b[0m\u001b[32m\"Attention Is All You Need\"\u001b[0m\u001b[1;38;2;118;185;0m differs from existing sequence \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mtransduction models because it is the first transduction model that relies entirely on self-attention to compute \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mrepresentations of its input and output, without using sequence-aligned RNNs \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mRecurrent Neural Networks\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m or \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mconvolution. Most competitive neural sequence transduction models have an encoder-decoder structure, where the \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mencoder maps an input sequence of symbol representations to a sequence of continuous representations, and then the \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mdecoder generates an output sequence of symbols one element at a time.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mThe Transformer model replaces the recurrent layers most commonly used in encoder-decoder architectures with \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mmulti-headed self-attention. This architecture allows the model to be trained significantly faster than \u001b[0m\n",
       "\u001b[1;38;2;118;185;0marchitectures based on recurrent or convolutional layers. The model has been applied to machine translation tasks \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mand has achieved a new state of the art on both WMT \u001b[0m\u001b[1;36m2014\u001b[0m\u001b[1;38;2;118;185;0m English-to-German and WMT \u001b[0m\u001b[1;36m2014\u001b[0m\u001b[1;38;2;118;185;0m English-to-French \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mtranslation tasks. In the former task, the best model outperforms even all previously reported ensembles.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mThe Transformer model also generalizes well to other tasks, such as English constituency parsing. The authors show \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mthat the model can be successfully applied to English constituency parsing both with large and limited training \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mdata. The model's performance on these tasks suggests that attention-based models have the potential to be applied \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mto a wide range of tasks beyond machine translation.\u001b[0m\n",
       "\n",
       "\u001b[1;35mReference\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0ms\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m:\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;38;2;118;185;0mQuote from Attention Is All You Need\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\u001b[1;38;2;118;185;0m . \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;36m2023\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m. Attention Is All You Need. Retrieved from \u001b[0m\n",
       "\u001b[1;38;2;118;185;0m<\u001b[0m\u001b[4;94mhttps:\u001b[0m\u001b[4;94m//arxiv.org/pdf/1706.03762.pdf\u001b[0m\u001b[1;39m>\u001b[0m\n",
       "\n",
       "\u001b[1;39mSummary:\u001b[0m\n",
       "\u001b[1;39mIn \u001b[0m\u001b[32m\"Attention Is All You Need,\"\u001b[0m\u001b[1;39m the authors propose the Transformer model, a new simple network architecture based \u001b[0m\n",
       "\u001b[1;39msolely on attention mechanisms, dispensing with recurrence and convolutions entirely. The model has been shown to \u001b[0m\n",
       "\u001b[1;39mbe superior in quality while being more parallelizable and requiring significantly less time to train. It has \u001b[0m\n",
       "\u001b[1;39machieved a new state of the art on machine translation tasks and has been successfully applied to English \u001b[0m\n",
       "\u001b[1;39mconstituency parsing with both large and limited training data.\u001b[0m\n",
       "\n",
       "\u001b[1;35mSource\u001b[0m\u001b[1;39m(\u001b[0m\u001b[1;39ms\u001b[0m\u001b[1;39m)\u001b[0m\u001b[1;39m:\u001b[0m\n",
       "\u001b[32m\"Attention Is All You Need\"\u001b[0m\u001b[1;39m \u001b[0m\u001b[1;39m(\u001b[0m\u001b[1;36m2023\u001b[0m\u001b[1;39m)\u001b[0m\u001b[1;39m, A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. \u001b[0m\n",
       "\u001b[1;39mKaiser, and I. Polosukhin, <\u001b[0m\u001b[4;94mhttps://arxiv.org/pdf/1706.03762.pdf\u001b[0m\u001b[1;38;2;118;185;0m>\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">QA Pair </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>\n",
       "<span style=\"font-weight: bold\">Question: What are the inherent limitations of current Large Language Models (LMs) and how does the proposed </span>\n",
       "<span style=\"font-weight: bold\">neuro-symbolic architecture, the Modular Reasoning, Knowledge and Language (MRKL) system, aim to address these </span>\n",
       "<span style=\"font-weight: bold\">limitations? How does this system compare in performance to other sequence transduction models like the Transformer</span>\n",
       "<span style=\"font-weight: bold\">model?</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mQA Pair \u001b[0m\u001b[1;36m3\u001b[0m\n",
       "\u001b[1mQuestion: What are the inherent limitations of current Large Language Models \u001b[0m\u001b[1m(\u001b[0m\u001b[1mLMs\u001b[0m\u001b[1m)\u001b[0m\u001b[1m and how does the proposed \u001b[0m\n",
       "\u001b[1mneuro-symbolic architecture, the Modular Reasoning, Knowledge and Language \u001b[0m\u001b[1m(\u001b[0m\u001b[1mMRKL\u001b[0m\u001b[1m)\u001b[0m\u001b[1m system, aim to address these \u001b[0m\n",
       "\u001b[1mlimitations? How does this system compare in performance to other sequence transduction models like the Transformer\u001b[0m\n",
       "\u001b[1mmodel?\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">RAG Answer: Great question! According to the paper </span><span style=\"color: #008000; text-decoration-color: #008000\">\"MRKL Systems: A modular, neuro-symbolic architecture that </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">combines large language models, external knowledge sources and discrete reasoning\"</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> published on May </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">, </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2022</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">, Large </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Language Models (LMs) are an essential backbone of any future AI system, but they have inherent limitations.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">One limitation is that it is not practical to fine-tune and serve multiple large models. Furthermore, catastrophic </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">forgetting makes it difficult to fine-tune a multi-task-trained LM on a new task that hadn't been covered in its </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">training. This necessitates retraining on the entire task set, which is infeasible due to the cost of training such</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">models.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">The proposed neuro-symbolic architecture, the Modular Reasoning, Knowledge and Language (MRKL) system, aims to </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">address these limitations by adopting a systems approach. The MRKL system conceptualizes the challenge as one that </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">involves knowledge and reasoning in addition to linguistic processing. It defines a flexible architecture with </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">multiple neural models, complemented by discrete knowledge and reasoning modules. This approach enables the MRKL </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">system to enjoy the benefits of self-supervised deep language models without suffering from the drawbacks mentioned</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">above.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Regarding the performance of the MRKL system compared to other sequence transduction models like the Transformer </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">model, the paper does not provide a direct comparison. However, the MRKL system is designed to be more flexible and</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">modular than the Transformer architecture, which should enable it to handle a wider range of tasks and accommodate </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">new tasks more easily. Additionally, the integration of external knowledge sources and discrete reasoning modules </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">should enable the MRKL system to make more informed and logical decisions.</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mRAG Answer: Great question! According to the paper \u001b[0m\u001b[32m\"MRKL Systems: A modular, neuro-symbolic architecture that \u001b[0m\n",
       "\u001b[32mcombines large language models, external knowledge sources and discrete reasoning\"\u001b[0m\u001b[1;38;2;118;185;0m published on May \u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;38;2;118;185;0m, \u001b[0m\u001b[1;36m2022\u001b[0m\u001b[1;38;2;118;185;0m, Large \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mLanguage Models \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mLMs\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m are an essential backbone of any future AI system, but they have inherent limitations.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mOne limitation is that it is not practical to fine-tune and serve multiple large models. Furthermore, catastrophic \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mforgetting makes it difficult to fine-tune a multi-task-trained LM on a new task that hadn't been covered in its \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mtraining. This necessitates retraining on the entire task set, which is infeasible due to the cost of training such\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mmodels.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mThe proposed neuro-symbolic architecture, the Modular Reasoning, Knowledge and Language \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mMRKL\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m system, aims to \u001b[0m\n",
       "\u001b[1;38;2;118;185;0maddress these limitations by adopting a systems approach. The MRKL system conceptualizes the challenge as one that \u001b[0m\n",
       "\u001b[1;38;2;118;185;0minvolves knowledge and reasoning in addition to linguistic processing. It defines a flexible architecture with \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mmultiple neural models, complemented by discrete knowledge and reasoning modules. This approach enables the MRKL \u001b[0m\n",
       "\u001b[1;38;2;118;185;0msystem to enjoy the benefits of self-supervised deep language models without suffering from the drawbacks mentioned\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mabove.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mRegarding the performance of the MRKL system compared to other sequence transduction models like the Transformer \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mmodel, the paper does not provide a direct comparison. However, the MRKL system is designed to be more flexible and\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mmodular than the Transformer architecture, which should enable it to handle a wider range of tasks and accommodate \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mnew tasks more easily. Additionally, the integration of external knowledge sources and discrete reasoning modules \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mshould enable the MRKL system to make more informed and logical decisions.\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## TODO: Generate some synthetic answers to the questions above.\n",
    "##   Try to use the same syntax as the cell above\n",
    "rag_answers = []\n",
    "for i, q in enumerate(synth_questions):\n",
    "    ## TODO: Compute the RAG Answer\n",
    "    rag_answer_tokens = list(rag_chain.stream(q))\n",
    "    rag_answer = \"\".join(token['output'] for token in rag_answer_tokens if 'output' in token)\n",
    "    rag_answers += [rag_answer]\n",
    "    pprint2(f\"QA Pair {i+1}\", q, \"\", sep=\"\\n\")\n",
    "    pprint(f\"RAG Answer: {rag_answer}\", \"\", sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Ho5cnN_Xt_yr",
   "metadata": {
    "id": "Ho5cnN_Xt_yr"
   },
   "source": [
    "<br>\n",
    "\n",
    "### **Step 5:** Implement A Human Preference Metric\n",
    "\n",
    "In this section, we can implement the fourth part of our evaluation routine:\n",
    "\n",
    "- Sample the RAG agent document pool to find two document chunks.\n",
    "- Use those two document chunks to generate a synthetic \"baseline\" question-answer pair.\n",
    "- Use the RAG agent to generate its own answer.\n",
    "- **Use a judge LLM to compare the two responses while grounding the synthetic generation as \"ground-truth correct.\"**\n",
    "\n",
    "The chain should be a simple but powerful process that tests for the following objective:\n",
    "\n",
    "> Does my RAG chain outperform a narrow chatbot with limited document access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "sf6f2oFLuPtu",
   "metadata": {
    "id": "sf6f2oFLuPtu"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Set </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>\n",
       "\n",
       "<span style=\"font-weight: bold\">Question: Question: How does Mistral 7B improve performance and efficiency compared to other language models, such </span>\n",
       "<span style=\"font-weight: bold\">as Llama </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\"> 13B and Llama </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\"> 34B? Additionally, how can large language model (LLM) based chat assistants be </span>\n",
       "<span style=\"font-weight: bold\">effectively evaluated using strong LLMs as judges?</span>\n",
       "\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mSet \u001b[0m\u001b[1;36m1\u001b[0m\n",
       "\n",
       "\u001b[1mQuestion: Question: How does Mistral 7B improve performance and efficiency compared to other language models, such \u001b[0m\n",
       "\u001b[1mas Llama \u001b[0m\u001b[1;36m2\u001b[0m\u001b[1m 13B and Llama \u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m 34B? Additionally, how can large language model \u001b[0m\u001b[1m(\u001b[0m\u001b[1mLLM\u001b[0m\u001b[1m)\u001b[0m\u001b[1m based chat assistants be \u001b[0m\n",
       "\u001b[1meffectively evaluated using strong LLMs as judges?\u001b[0m\n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Synth Answer: Answer: Mistral 7B outperforms Llama </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> 13B across all evaluated benchmarks and Llama </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> 34B in </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">reasoning, mathematics, and code generation. It leverages grouped-query attention (GQA) for faster inference and </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">sliding window attention (SWA) to handle sequences of arbitrary length with reduced inference cost. The model's </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">architecture includes a transformer with the following parameters: dim (</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4096</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">), n_layers (</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">32</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">), head_dim (</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">128</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">), </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">hidden_dim (</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">14336</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">), n_heads (</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">32</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">), n_kv_heads (</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">), window_size (</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4096</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">), context_len (</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8192</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">), and vocab_size (</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">32000</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">).</span>\n",
       "\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mSynth Answer: Answer: Mistral 7B outperforms Llama \u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;38;2;118;185;0m 13B across all evaluated benchmarks and Llama \u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;38;2;118;185;0m 34B in \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mreasoning, mathematics, and code generation. It leverages grouped-query attention \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mGQA\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m for faster inference and \u001b[0m\n",
       "\u001b[1;38;2;118;185;0msliding window attention \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mSWA\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m to handle sequences of arbitrary length with reduced inference cost. The model's \u001b[0m\n",
       "\u001b[1;38;2;118;185;0marchitecture includes a transformer with the following parameters: dim \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;36m4096\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m, n_layers \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m, head_dim \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m, \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mhidden_dim \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;36m14336\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m, n_heads \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m, n_kv_heads \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m, window_size \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;36m4096\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m, context_len \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;36m8192\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m, and vocab_size \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;36m32000\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m.\u001b[0m\n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">RAG Answer: Based on the provided document, Mistral 7B improves performance and efficiency compared to other </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">language models such as Llama </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> 13B and Llama </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> 34B in several ways.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Firstly, Mistral 7B is a language model with </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> billion parameters that is engineered for superior performance and </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">efficiency. It outperforms the best open 13B model (Llama </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">) across all evaluated benchmarks and the best released </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">34B model (Llama </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">) in reasoning, mathematics, and code generation (Source: Mistral 7B).</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Secondly, Mistral 7B leverages grouped-query attention (GQA) for faster inference, and sliding window attention </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(SWA) to effectively handle sequences of arbitrary length with a reduced inference cost. This makes it more </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">efficient than other models (Source: Mistral 7B).</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">As for evaluating large language model (LLM) based chat assistants, it can be challenging due to their broad </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">capabilities and the inadequacy of existing benchmarks in measuring human preferences. However, the usage of strong</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">LLMs as judges to evaluate these models on more open-ended questions has been explored. This method, known as </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">LLM-as-a-judge, examines the usage and limitations of this approach, including position, verbosity, and </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">self-enhancement biases, as well as limited reasoning ability. Solutions to mitigate some of these issues have also</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">been proposed (Source: Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena).</span>\n",
       "\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mRAG Answer: Based on the provided document, Mistral 7B improves performance and efficiency compared to other \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mlanguage models such as Llama \u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;38;2;118;185;0m 13B and Llama \u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;38;2;118;185;0m 34B in several ways.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mFirstly, Mistral 7B is a language model with \u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;38;2;118;185;0m billion parameters that is engineered for superior performance and \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mefficiency. It outperforms the best open 13B model \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mLlama \u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m across all evaluated benchmarks and the best released \u001b[0m\n",
       "\u001b[1;38;2;118;185;0m34B model \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mLlama \u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m in reasoning, mathematics, and code generation \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mSource: Mistral 7B\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mSecondly, Mistral 7B leverages grouped-query attention \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mGQA\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m for faster inference, and sliding window attention \u001b[0m\n",
       "\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mSWA\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m to effectively handle sequences of arbitrary length with a reduced inference cost. This makes it more \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mefficient than other models \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mSource: Mistral 7B\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mAs for evaluating large language model \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mLLM\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m based chat assistants, it can be challenging due to their broad \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mcapabilities and the inadequacy of existing benchmarks in measuring human preferences. However, the usage of strong\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mLLMs as judges to evaluate these models on more open-ended questions has been explored. This method, known as \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mLLM-as-a-judge, examines the usage and limitations of this approach, including position, verbosity, and \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mself-enhancement biases, as well as limited reasoning ability. Solutions to mitigate some of these issues have also\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mbeen proposed \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mSource: Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m.\u001b[0m\n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Synth Evaluation: [</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">] The second answer is better than the first and does not introduce any inconsistencies.</span>\n",
       "<span style=\"font-weight: bold\">Justification: The second answer not only includes the same information as the first answer regarding Mistral 7B's </span>\n",
       "<span style=\"font-weight: bold\">improvements in performance and efficiency compared to other language models, but it also provides additional </span>\n",
       "<span style=\"font-weight: bold\">context and explanation, such as the fact that Mistral 7B is a language model with </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span><span style=\"font-weight: bold\"> billion parameters and the </span>\n",
       "<span style=\"font-weight: bold\">reasoning behind using strong LLMs as judges for evaluating other large language models. The second answer also </span>\n",
       "<span style=\"font-weight: bold\">includes citations to its sources, providing additional credibility to the information presented.</span>\n",
       "\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mSynth Evaluation: \u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1m]\u001b[0m\u001b[1m The second answer is better than the first and does not introduce any inconsistencies.\u001b[0m\n",
       "\u001b[1mJustification: The second answer not only includes the same information as the first answer regarding Mistral 7B's \u001b[0m\n",
       "\u001b[1mimprovements in performance and efficiency compared to other language models, but it also provides additional \u001b[0m\n",
       "\u001b[1mcontext and explanation, such as the fact that Mistral 7B is a language model with \u001b[0m\u001b[1;36m7\u001b[0m\u001b[1m billion parameters and the \u001b[0m\n",
       "\u001b[1mreasoning behind using strong LLMs as judges for evaluating other large language models. The second answer also \u001b[0m\n",
       "\u001b[1mincludes citations to its sources, providing additional credibility to the information presented.\u001b[0m\n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Set </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>\n",
       "\n",
       "<span style=\"font-weight: bold\">Question: Question: How does the proposed Transformer model in </span><span style=\"color: #008000; text-decoration-color: #008000\">\"Attention Is All You Need\"</span><span style=\"font-weight: bold\"> differ from the existing</span>\n",
       "<span style=\"font-weight: bold\">sequence transduction models, and what improvements does it bring to machine translation tasks? How well does it </span>\n",
       "<span style=\"font-weight: bold\">generalize to other tasks like English constituency parsing?</span>\n",
       "\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mSet \u001b[0m\u001b[1;36m2\u001b[0m\n",
       "\n",
       "\u001b[1mQuestion: Question: How does the proposed Transformer model in \u001b[0m\u001b[32m\"Attention Is All You Need\"\u001b[0m\u001b[1m differ from the existing\u001b[0m\n",
       "\u001b[1msequence transduction models, and what improvements does it bring to machine translation tasks? How well does it \u001b[0m\n",
       "\u001b[1mgeneralize to other tasks like English constituency parsing?\u001b[0m\n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Synth Answer: Answer: The proposed Transformer model is a new simple network architecture based solely on attention</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">mechanisms, dispensing with recurrence and convolutions entirely. Unlike the dominant sequence transduction models </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">which are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration, the </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Transformer model connects the encoder and decoder through an attention mechanism. Experiments on two machine </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">translation tasks, WMT </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2014</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> English-to-German and English-to-French translation tasks, show that the Transformer </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">models are superior in quality while being more parallelizable and requiring significantly less time to train. The </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Transformer model achieves </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">28.4</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> BLEU on the WMT </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2014</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> English-to-German translation task, improving over the </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">existing best results by over </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> BLEU, and establishes a new single-model state-of-the-art BLEU score of </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">41.8</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> after </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">training for </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3.5</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> days on eight GPUs for the English-to-French translation task. The Transformer generalizes well to</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">other tasks such as English constituency parsing with large and limited training data.</span>\n",
       "\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mSynth Answer: Answer: The proposed Transformer model is a new simple network architecture based solely on attention\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mmechanisms, dispensing with recurrence and convolutions entirely. Unlike the dominant sequence transduction models \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mwhich are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration, the \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mTransformer model connects the encoder and decoder through an attention mechanism. Experiments on two machine \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mtranslation tasks, WMT \u001b[0m\u001b[1;36m2014\u001b[0m\u001b[1;38;2;118;185;0m English-to-German and English-to-French translation tasks, show that the Transformer \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mmodels are superior in quality while being more parallelizable and requiring significantly less time to train. The \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mTransformer model achieves \u001b[0m\u001b[1;36m28.4\u001b[0m\u001b[1;38;2;118;185;0m BLEU on the WMT \u001b[0m\u001b[1;36m2014\u001b[0m\u001b[1;38;2;118;185;0m English-to-German translation task, improving over the \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mexisting best results by over \u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;38;2;118;185;0m BLEU, and establishes a new single-model state-of-the-art BLEU score of \u001b[0m\u001b[1;36m41.8\u001b[0m\u001b[1;38;2;118;185;0m after \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mtraining for \u001b[0m\u001b[1;36m3.5\u001b[0m\u001b[1;38;2;118;185;0m days on eight GPUs for the English-to-French translation task. The Transformer generalizes well to\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mother tasks such as English constituency parsing with large and limited training data.\u001b[0m\n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">RAG Answer: The Transformer model proposed in </span><span style=\"color: #008000; text-decoration-color: #008000\">\"Attention Is All You Need\"</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> differs from existing sequence </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">transduction models because it is the first transduction model that relies entirely on self-attention to compute </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">representations of its input and output, without using sequence-aligned RNNs (Recurrent Neural Networks) or </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">convolution. Most competitive neural sequence transduction models have an encoder-decoder structure, where the </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">encoder maps an input sequence of symbol representations to a sequence of continuous representations, and then the </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">decoder generates an output sequence of symbols one element at a time.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">The Transformer model replaces the recurrent layers most commonly used in encoder-decoder architectures with </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">multi-headed self-attention. This architecture allows the model to be trained significantly faster than </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">architectures based on recurrent or convolutional layers. The model has been applied to machine translation tasks </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">and has achieved a new state of the art on both WMT </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2014</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> English-to-German and WMT </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2014</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> English-to-French </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">translation tasks. In the former task, the best model outperforms even all previously reported ensembles.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">The Transformer model also generalizes well to other tasks, such as English constituency parsing. The authors show </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">that the model can be successfully applied to English constituency parsing both with large and limited training </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">data. The model's performance on these tasks suggests that attention-based models have the potential to be applied </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">to a wide range of tasks beyond machine translation.</span>\n",
       "\n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Reference</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(s):</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">[Quote from Attention Is All You Need] . (</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2023</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">). Attention Is All You Need. Retrieved from </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">&lt;</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://arxiv.org/pdf/1706.03762.pdf</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">&gt;</span>\n",
       "\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">Summary:</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">In </span><span style=\"color: #008000; text-decoration-color: #008000\">\"Attention Is All You Need,\"</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\"> the authors propose the Transformer model, a new simple network architecture based </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">solely on attention mechanisms, dispensing with recurrence and convolutions entirely. The model has been shown to </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">be superior in quality while being more parallelizable and requiring significantly less time to train. It has </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">achieved a new state of the art on machine translation tasks and has been successfully applied to English </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">constituency parsing with both large and limited training data.</span>\n",
       "\n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Source</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(s):</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\"Attention Is All You Need\"</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\"> (</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2023</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">), A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">Kaiser, and I. Polosukhin, &lt;</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://arxiv.org/pdf/1706.03762.pdf</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">&gt;</span>\n",
       "\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mRAG Answer: The Transformer model proposed in \u001b[0m\u001b[32m\"Attention Is All You Need\"\u001b[0m\u001b[1;38;2;118;185;0m differs from existing sequence \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mtransduction models because it is the first transduction model that relies entirely on self-attention to compute \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mrepresentations of its input and output, without using sequence-aligned RNNs \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mRecurrent Neural Networks\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m or \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mconvolution. Most competitive neural sequence transduction models have an encoder-decoder structure, where the \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mencoder maps an input sequence of symbol representations to a sequence of continuous representations, and then the \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mdecoder generates an output sequence of symbols one element at a time.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mThe Transformer model replaces the recurrent layers most commonly used in encoder-decoder architectures with \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mmulti-headed self-attention. This architecture allows the model to be trained significantly faster than \u001b[0m\n",
       "\u001b[1;38;2;118;185;0marchitectures based on recurrent or convolutional layers. The model has been applied to machine translation tasks \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mand has achieved a new state of the art on both WMT \u001b[0m\u001b[1;36m2014\u001b[0m\u001b[1;38;2;118;185;0m English-to-German and WMT \u001b[0m\u001b[1;36m2014\u001b[0m\u001b[1;38;2;118;185;0m English-to-French \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mtranslation tasks. In the former task, the best model outperforms even all previously reported ensembles.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mThe Transformer model also generalizes well to other tasks, such as English constituency parsing. The authors show \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mthat the model can be successfully applied to English constituency parsing both with large and limited training \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mdata. The model's performance on these tasks suggests that attention-based models have the potential to be applied \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mto a wide range of tasks beyond machine translation.\u001b[0m\n",
       "\n",
       "\u001b[1;35mReference\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0ms\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m:\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;38;2;118;185;0mQuote from Attention Is All You Need\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\u001b[1;38;2;118;185;0m . \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;36m2023\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m. Attention Is All You Need. Retrieved from \u001b[0m\n",
       "\u001b[1;38;2;118;185;0m<\u001b[0m\u001b[4;94mhttps:\u001b[0m\u001b[4;94m//arxiv.org/pdf/1706.03762.pdf\u001b[0m\u001b[1;39m>\u001b[0m\n",
       "\n",
       "\u001b[1;39mSummary:\u001b[0m\n",
       "\u001b[1;39mIn \u001b[0m\u001b[32m\"Attention Is All You Need,\"\u001b[0m\u001b[1;39m the authors propose the Transformer model, a new simple network architecture based \u001b[0m\n",
       "\u001b[1;39msolely on attention mechanisms, dispensing with recurrence and convolutions entirely. The model has been shown to \u001b[0m\n",
       "\u001b[1;39mbe superior in quality while being more parallelizable and requiring significantly less time to train. It has \u001b[0m\n",
       "\u001b[1;39machieved a new state of the art on machine translation tasks and has been successfully applied to English \u001b[0m\n",
       "\u001b[1;39mconstituency parsing with both large and limited training data.\u001b[0m\n",
       "\n",
       "\u001b[1;35mSource\u001b[0m\u001b[1;39m(\u001b[0m\u001b[1;39ms\u001b[0m\u001b[1;39m)\u001b[0m\u001b[1;39m:\u001b[0m\n",
       "\u001b[32m\"Attention Is All You Need\"\u001b[0m\u001b[1;39m \u001b[0m\u001b[1;39m(\u001b[0m\u001b[1;36m2023\u001b[0m\u001b[1;39m)\u001b[0m\u001b[1;39m, A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. \u001b[0m\n",
       "\u001b[1;39mKaiser, and I. Polosukhin, <\u001b[0m\u001b[4;94mhttps://arxiv.org/pdf/1706.03762.pdf\u001b[0m\u001b[1;38;2;118;185;0m>\u001b[0m\n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Synth Evaluation: [</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">] Justification: Both answers provide a thorough explanation of how the proposed Transformer </span>\n",
       "<span style=\"font-weight: bold\">model differs from existing sequence transduction models and highlight its improvements in machine translation </span>\n",
       "<span style=\"font-weight: bold\">tasks. However, Answer </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\"> is more precise in its details and provides specific BLEU scores achieved by the model in </span>\n",
       "<span style=\"font-weight: bold\">the WMT </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2014</span><span style=\"font-weight: bold\"> English-to-German and English-to-French translation tasks. Answer </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">, while accurate, does not provide </span>\n",
       "<span style=\"font-weight: bold\">these specific details. Therefore, Answer </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\"> is more informative and specific, making it superior to Answer </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">.</span>\n",
       "\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mSynth Evaluation: \u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m]\u001b[0m\u001b[1m Justification: Both answers provide a thorough explanation of how the proposed Transformer \u001b[0m\n",
       "\u001b[1mmodel differs from existing sequence transduction models and highlight its improvements in machine translation \u001b[0m\n",
       "\u001b[1mtasks. However, Answer \u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m is more precise in its details and provides specific BLEU scores achieved by the model in \u001b[0m\n",
       "\u001b[1mthe WMT \u001b[0m\u001b[1;36m2014\u001b[0m\u001b[1m English-to-German and English-to-French translation tasks. Answer \u001b[0m\u001b[1;36m2\u001b[0m\u001b[1m, while accurate, does not provide \u001b[0m\n",
       "\u001b[1mthese specific details. Therefore, Answer \u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m is more informative and specific, making it superior to Answer \u001b[0m\u001b[1;36m2\u001b[0m\u001b[1m.\u001b[0m\n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Set </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>\n",
       "\n",
       "<span style=\"font-weight: bold\">Question: Question: What are the inherent limitations of current Large Language Models (LMs) and how does the </span>\n",
       "<span style=\"font-weight: bold\">proposed neuro-symbolic architecture, the Modular Reasoning, Knowledge and Language (MRKL) system, aim to address </span>\n",
       "<span style=\"font-weight: bold\">these limitations? How does this system compare in performance to other sequence transduction models like the </span>\n",
       "<span style=\"font-weight: bold\">Transformer model?</span>\n",
       "\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mSet \u001b[0m\u001b[1;36m3\u001b[0m\n",
       "\n",
       "\u001b[1mQuestion: Question: What are the inherent limitations of current Large Language Models \u001b[0m\u001b[1m(\u001b[0m\u001b[1mLMs\u001b[0m\u001b[1m)\u001b[0m\u001b[1m and how does the \u001b[0m\n",
       "\u001b[1mproposed neuro-symbolic architecture, the Modular Reasoning, Knowledge and Language \u001b[0m\u001b[1m(\u001b[0m\u001b[1mMRKL\u001b[0m\u001b[1m)\u001b[0m\u001b[1m system, aim to address \u001b[0m\n",
       "\u001b[1mthese limitations? How does this system compare in performance to other sequence transduction models like the \u001b[0m\n",
       "\u001b[1mTransformer model?\u001b[0m\n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Synth Answer: Answer: The inherent limitations of current Large Language Models (LMs) are that despite their </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">versatility and impressive capabilities, their output is often wrong or ridiculously incorrect. This is exemplified</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">by the sample output of GPT-</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> on simple queries. The proposed neuro-symbolic architecture, the Modular Reasoning, </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Knowledge and Language (MRKL) system, aims to address these limitations by adopting a systems approach that </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">involves knowledge and reasoning in addition to linguistic processing. The MRKL system is a flexible architecture </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">with multiple neural models, complemented by discrete knowledge and reasoning modules. In terms of performance, the</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Transformer model, a sequence transduction model based solely on attention mechanisms, has been shown to be </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">superior in quality while being more parallelizable and requiring significantly less time to train. However, the </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">MRKL system's performance is not compared to the Transformer model in the provided document.</span>\n",
       "\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mSynth Answer: Answer: The inherent limitations of current Large Language Models \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mLMs\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m are that despite their \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mversatility and impressive capabilities, their output is often wrong or ridiculously incorrect. This is exemplified\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mby the sample output of GPT-\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;38;2;118;185;0m on simple queries. The proposed neuro-symbolic architecture, the Modular Reasoning, \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mKnowledge and Language \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mMRKL\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m system, aims to address these limitations by adopting a systems approach that \u001b[0m\n",
       "\u001b[1;38;2;118;185;0minvolves knowledge and reasoning in addition to linguistic processing. The MRKL system is a flexible architecture \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mwith multiple neural models, complemented by discrete knowledge and reasoning modules. In terms of performance, the\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mTransformer model, a sequence transduction model based solely on attention mechanisms, has been shown to be \u001b[0m\n",
       "\u001b[1;38;2;118;185;0msuperior in quality while being more parallelizable and requiring significantly less time to train. However, the \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mMRKL system's performance is not compared to the Transformer model in the provided document.\u001b[0m\n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">RAG Answer: Great question! According to the paper </span><span style=\"color: #008000; text-decoration-color: #008000\">\"MRKL Systems: A modular, neuro-symbolic architecture that </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">combines large language models, external knowledge sources and discrete reasoning\"</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> published on May </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">, </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2022</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">, Large </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Language Models (LMs) are an essential backbone of any future AI system, but they have inherent limitations.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">One limitation is that it is not practical to fine-tune and serve multiple large models. Furthermore, catastrophic </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">forgetting makes it difficult to fine-tune a multi-task-trained LM on a new task that hadn't been covered in its </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">training. This necessitates retraining on the entire task set, which is infeasible due to the cost of training such</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">models.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">The proposed neuro-symbolic architecture, the Modular Reasoning, Knowledge and Language (MRKL) system, aims to </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">address these limitations by adopting a systems approach. The MRKL system conceptualizes the challenge as one that </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">involves knowledge and reasoning in addition to linguistic processing. It defines a flexible architecture with </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">multiple neural models, complemented by discrete knowledge and reasoning modules. This approach enables the MRKL </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">system to enjoy the benefits of self-supervised deep language models without suffering from the drawbacks mentioned</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">above.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Regarding the performance of the MRKL system compared to other sequence transduction models like the Transformer </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">model, the paper does not provide a direct comparison. However, the MRKL system is designed to be more flexible and</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">modular than the Transformer architecture, which should enable it to handle a wider range of tasks and accommodate </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">new tasks more easily. Additionally, the integration of external knowledge sources and discrete reasoning modules </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">should enable the MRKL system to make more informed and logical decisions.</span>\n",
       "\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mRAG Answer: Great question! According to the paper \u001b[0m\u001b[32m\"MRKL Systems: A modular, neuro-symbolic architecture that \u001b[0m\n",
       "\u001b[32mcombines large language models, external knowledge sources and discrete reasoning\"\u001b[0m\u001b[1;38;2;118;185;0m published on May \u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;38;2;118;185;0m, \u001b[0m\u001b[1;36m2022\u001b[0m\u001b[1;38;2;118;185;0m, Large \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mLanguage Models \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mLMs\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m are an essential backbone of any future AI system, but they have inherent limitations.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mOne limitation is that it is not practical to fine-tune and serve multiple large models. Furthermore, catastrophic \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mforgetting makes it difficult to fine-tune a multi-task-trained LM on a new task that hadn't been covered in its \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mtraining. This necessitates retraining on the entire task set, which is infeasible due to the cost of training such\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mmodels.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mThe proposed neuro-symbolic architecture, the Modular Reasoning, Knowledge and Language \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mMRKL\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m system, aims to \u001b[0m\n",
       "\u001b[1;38;2;118;185;0maddress these limitations by adopting a systems approach. The MRKL system conceptualizes the challenge as one that \u001b[0m\n",
       "\u001b[1;38;2;118;185;0minvolves knowledge and reasoning in addition to linguistic processing. It defines a flexible architecture with \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mmultiple neural models, complemented by discrete knowledge and reasoning modules. This approach enables the MRKL \u001b[0m\n",
       "\u001b[1;38;2;118;185;0msystem to enjoy the benefits of self-supervised deep language models without suffering from the drawbacks mentioned\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mabove.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mRegarding the performance of the MRKL system compared to other sequence transduction models like the Transformer \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mmodel, the paper does not provide a direct comparison. However, the MRKL system is designed to be more flexible and\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mmodular than the Transformer architecture, which should enable it to handle a wider range of tasks and accommodate \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mnew tasks more easily. Additionally, the integration of external knowledge sources and discrete reasoning modules \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mshould enable the MRKL system to make more informed and logical decisions.\u001b[0m\n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Synth Evaluation: [</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">] The second answer is better than the first and does not introduce any inconsistencies.</span>\n",
       "<span style=\"font-weight: bold\">Justification: The second answer provides a more detailed and specific explanation of the inherent limitations of </span>\n",
       "<span style=\"font-weight: bold\">Large Language Models, citing the impracticality of fine-tuning and serving multiple large models and the challenge</span>\n",
       "<span style=\"font-weight: bold\">of catastrophic forgetting. It also offers a more comprehensive description of how the proposed MRKL system aims to</span>\n",
       "<span style=\"font-weight: bold\">address these limitations. While neither answer offers a direct comparison of the MRKL system's performance to the </span>\n",
       "<span style=\"font-weight: bold\">Transformer model, the second answer suggests that the MRKL system's flexibility and modularity may enable it to </span>\n",
       "<span style=\"font-weight: bold\">handle a wider range of tasks and make more informed decisions.</span>\n",
       "\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mSynth Evaluation: \u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1m]\u001b[0m\u001b[1m The second answer is better than the first and does not introduce any inconsistencies.\u001b[0m\n",
       "\u001b[1mJustification: The second answer provides a more detailed and specific explanation of the inherent limitations of \u001b[0m\n",
       "\u001b[1mLarge Language Models, citing the impracticality of fine-tuning and serving multiple large models and the challenge\u001b[0m\n",
       "\u001b[1mof catastrophic forgetting. It also offers a more comprehensive description of how the proposed MRKL system aims to\u001b[0m\n",
       "\u001b[1maddress these limitations. While neither answer offers a direct comparison of the MRKL system's performance to the \u001b[0m\n",
       "\u001b[1mTransformer model, the second answer suggests that the MRKL system's flexibility and modularity may enable it to \u001b[0m\n",
       "\u001b[1mhandle a wider range of tasks and make more informed decisions.\u001b[0m\n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eval_prompt = ChatPromptTemplate.from_template(\"\"\"INSTRUCTION: \n",
    "Evaluate the following Question-Answer pair for human preference and consistency.\n",
    "Assume the first answer is a ground truth answer and has to be correct.\n",
    "Assume the second answer may or may not be true.\n",
    "[1] The second answer lies, does not answer the question, or is inferior to the first answer.\n",
    "[2] The second answer is better than the first and does not introduce any inconsistencies.\n",
    "\n",
    "Output Format:\n",
    "[Score] Justification\n",
    "\n",
    "{qa_trio}\n",
    "\n",
    "EVALUATION: \n",
    "\"\"\")\n",
    "\n",
    "pref_score = []\n",
    "\n",
    "trio_gen = zip(synth_questions, synth_answers, rag_answers)\n",
    "for i, (q, a_synth, a_rag) in enumerate(trio_gen):\n",
    "    pprint2(f\"Set {i+1}\\n\\nQuestion: {q}\\n\\n\")\n",
    "\n",
    "    qa_trio = f\"Question: {q}\\n\\nAnswer 1 (Ground Truth): {a_synth}\\n\\n Answer 2 (New Answer): {a_rag}\"\n",
    "    pref_score += [(eval_prompt | llm).invoke({'qa_trio': qa_trio})]\n",
    "    pprint(f\"Synth Answer: {a_synth}\\n\\n\")\n",
    "    pprint(f\"RAG Answer: {a_rag}\\n\\n\")\n",
    "    pprint2(f\"Synth Evaluation: {pref_score[-1]}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6595662-9f49-44eb-9868-2a3fdb1fb60f",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "**Congratulations! We now have an LLM system that reasons about our pipeline and tries to evaluate it!** Now that we have some judge results, we can simply aggregate the results and see how often our formulation was according to an LLM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3L_q6fMH3i6_",
   "metadata": {
    "id": "3L_q6fMH3i6_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preference Score: 0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "pref_score = sum((\"[2]\" in score) for score in pref_score) / len(pref_score)\n",
    "print(f\"Preference Score: {pref_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf80bf04-118d-44a2-a740-361a756a1d5f",
   "metadata": {
    "id": "cf80bf04-118d-44a2-a740-361a756a1d5f"
   },
   "source": [
    "----\n",
    "\n",
    "<br>\n",
    "\n",
    "## **Part 4:** Advanced Formulations\n",
    "\n",
    "The exercise above was meant to prepare you for the final assessment of the course and showcased a simple but effective evaluator chain. The objective and implementation details were provided for you, and the logic for using it probably makes sense now that you've seen it in action. \n",
    "\n",
    "With that being said, this metric was merely a product of us specifying:\n",
    "- **What kind of behavior is important for our pipeline to have?**\n",
    "- **What do we need to do in order to exhibit and evaluate this behavior?**\n",
    "\n",
    "From these two questions, we could have come up with plenty of other evaluation metrics that could have assessed different attributes, incorporated different evaluator chain techniques, and even required different pipeline organization strategies. Though far from an exhaustive list, some common formulations you will likely come across may include:\n",
    "\n",
    "- **Style Evaluation:** Some evaluation formulations can be as simple as \"let me ask some questions and see if the output feels desirable.\" This might be used to see whether a chatbot \"acts like it's supposed to\" based on a description provided to a judge LLM. We're using quotations since this kind of assessment can reasonably be achieved with nothing but prompt engineering and a while loop.\n",
    "\n",
    "- **Ground-Truth Evaluation:** In our chain, we used synthetic generation to create some random questions and answers using a sampling strategy, but in reality you may actually have some representative questions and answers that you need your chatbot to consistently get right! In this case, a modification of the exercise chain above should be implemented and closely monitored as you develop your pipelines.\n",
    "\n",
    "- **Retrieval/Augmentation Evaluation:** This course made many assumptions about what kinds of preprocessing and prompting steps would be good for your pipelines, and much of this was determined by experimentation. Factors such as document preprocessing, chunking strategies, model selection, and prompt specification all played important roles, so creating metrics to validate these decisions may be of interest. This kind of metric might require your pipeline to output your context chunks or may even rely solely on embedding similarity comparisons, so keep this in mind when trying to implement a chain that works with multiple evaluation strategies. Consider the [**RagasEvaluatorChain**](https://docs.ragas.io/en/latest/howtos/integrations/langchain.html) abstraction as a decent starting point for making an custom generalizable evaluation routine. \n",
    "\n",
    "- **Trajectory Evaluation:** Using more advanced agent formulations, you can implement multiple-query strategies that assume the presence of conversational memory. With this, you can implement an evaluation agent which can:\n",
    "    - Ask a series of questions in order to evaluate how well the agent is able to adapt and cater to the scenario. This kind of system generally considers a series of correspondence and aims to tease out and evaluate a \"trajectory\" of how the agent navigated the conversation. The [**LangChain Trajectory Evaluators documentation**](https://python.langchain.com/docs/guides/evaluation/trajectory/) is a good starting point.\n",
    "    - Alternatively, you could also implement an evaluation agent that tries to achieve objectives by interacting with the chatbot. Such an agent can output whether they were able to navigate to their solution in a natural manner, and can even be used to generate a report about the percieved performance. The [**LangChain Agents documentation**](https://python.langchain.com/docs/modules/agents/concepts) is a good starting point!\n",
    "\n",
    "<br>\n",
    "\n",
    "At the end of the day, just make sure to use the tools you have at your disposal appropriately. By this point in the course, you should already be well-acquainted with the LLM core value propositions: **They're powerful, scalable, predictable, controllable, and orchestratable... but will act unpredictably when you just expect them to work by default.** Assess your needs, formulate and validate your pipelines, give enough information, and add as much control as you can to make your system work consistently, efficiently, and effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61faee2c-e534-4c89-91ae-45c37835dba5",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "<br>\n",
    "\n",
    "## **Part 5: [Assessment]** Evaluating For Credit\n",
    "\n",
    "Welcome to the last exercise of the course! Hopefully you've enjoyed the material and are ready to actually get credit for these notebooks! For this part:\n",
    "\n",
    "- **Make sure you're in the course environment**\n",
    "- **Make sure `docstore_index/` has been uploaded to the course environment...**\n",
    "    - **...and contains [at least one Arxiv paper](https://arxiv.org/search/advanced) which has been updated recently.**\n",
    "- **Make sure [`35_langserve.ipynb`](35_langserve.ipynb) is not occupying port 9012 with a running FastAPI service**\n",
    "\n",
    "**Objective:** On launch, [**`frontend/frontend_block.py`**](frontend/frontend_block.py) had several lines of code which trigger the course pass condition. Once the service was deemed healthy, that sequence of code was replaced with `## secret` by another microservice. Your objective is to invoke that series of commands by using your pipeline to pass the **Evaluation** check! Recall [`35_langserve.ipynb`](35_langserve.ipynb) and use it as a starting example! As a recommendation, consider duplicating it so that you can keep the original as an authoritative reference. \n",
    "\n",
    "**Once Finished:** While your course environment is still open, please navigate back to your course environment launcher area and click the **\"Assess Task\"** button! After that, you're all done!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "48e300ed-951c-4006-ac54-cbbd41251707",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "var url = 'http://'+window.location.host+'/8090';\n",
       "element.innerHTML = '<a style=\"color:green;\" target=\"_blank\" href='+url+'><h1>< Link To Gradio Frontend ></h1></a>';\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%js\n",
    "var url = 'http://'+window.location.host+'/8090';\n",
    "element.innerHTML = '<a style=\"color:green;\" target=\"_blank\" href='+url+'><h1>< Link To Gradio Frontend ></h1></a>';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4386a197-d5ba-4f35-80b7-9b9ae9fb1b05",
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectError",
     "evalue": "[Errno 111] Connection refused",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConnectError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/httpx/_transports/default.py:69\u001b[0m, in \u001b[0;36mmap_httpcore_exceptions\u001b[0;34m()\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 69\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/httpx/_transports/default.py:233\u001b[0m, in \u001b[0;36mHTTPTransport.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[0;32m--> 233\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py:216\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_connections(closing)\n\u001b[0;32m--> 216\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py:196\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/httpcore/_sync/connection.py:99\u001b[0m, in \u001b[0;36mHTTPConnection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connect_failed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 99\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection\u001b[38;5;241m.\u001b[39mhandle_request(request)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/httpcore/_sync/connection.py:76\u001b[0m, in \u001b[0;36mHTTPConnection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 76\u001b[0m     stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_connect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m     ssl_object \u001b[38;5;241m=\u001b[39m stream\u001b[38;5;241m.\u001b[39mget_extra_info(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mssl_object\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/httpcore/_sync/connection.py:122\u001b[0m, in \u001b[0;36mHTTPConnection._connect\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconnect_tcp\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request, kwargs) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[0;32m--> 122\u001b[0m     stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_network_backend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect_tcp\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    123\u001b[0m     trace\u001b[38;5;241m.\u001b[39mreturn_value \u001b[38;5;241m=\u001b[39m stream\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/httpcore/_backends/sync.py:205\u001b[0m, in \u001b[0;36mSyncBackend.connect_tcp\u001b[0;34m(self, host, port, timeout, local_address, socket_options)\u001b[0m\n\u001b[1;32m    200\u001b[0m exc_map: ExceptionMapping \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    201\u001b[0m     socket\u001b[38;5;241m.\u001b[39mtimeout: ConnectTimeout,\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;167;01mOSError\u001b[39;00m: ConnectError,\n\u001b[1;32m    203\u001b[0m }\n\u001b[0;32m--> 205\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmap_exceptions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexc_map\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[43m    \u001b[49m\u001b[43msock\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_connection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[43m        \u001b[49m\u001b[43maddress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    208\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    209\u001b[0m \u001b[43m        \u001b[49m\u001b[43msource_address\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msource_address\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    210\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/contextlib.py:158\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__exit__\u001b[0;34m(self, typ, value, traceback)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 158\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgen\u001b[38;5;241m.\u001b[39mthrow(typ, value, traceback)\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/httpcore/_exceptions.py:14\u001b[0m, in \u001b[0;36mmap_exceptions\u001b[0;34m(map)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(exc, from_exc):\n\u001b[0;32m---> 14\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m to_exc(exc) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mConnectError\u001b[0m: [Errno 111] Connection refused",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mConnectError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Invoke generator route --> http://0.0.0.0:9012/generator\u001b[39;00m\n\u001b[1;32m      4\u001b[0m RemoteRunnable_generator \u001b[38;5;241m=\u001b[39m RemoteRunnable(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp://0.0.0.0:9012/generator/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m \u001b[43mRemoteRunnable_generator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTell me something interesting\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/langserve/client.py:356\u001b[0m, in \u001b[0;36mRemoteRunnable.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs:\n\u001b[1;32m    355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkwargs not implemented yet.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 356\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_with_config\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_invoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/langchain_core/runnables/base.py:1626\u001b[0m, in \u001b[0;36mRunnable._call_with_config\u001b[0;34m(self, func, input, config, run_type, **kwargs)\u001b[0m\n\u001b[1;32m   1622\u001b[0m     context \u001b[38;5;241m=\u001b[39m copy_context()\n\u001b[1;32m   1623\u001b[0m     context\u001b[38;5;241m.\u001b[39mrun(var_child_runnable_config\u001b[38;5;241m.\u001b[39mset, child_config)\n\u001b[1;32m   1624\u001b[0m     output \u001b[38;5;241m=\u001b[39m cast(\n\u001b[1;32m   1625\u001b[0m         Output,\n\u001b[0;32m-> 1626\u001b[0m         \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1627\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcall_func_with_variable_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1628\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1629\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1630\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1631\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1632\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1633\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m   1634\u001b[0m     )\n\u001b[1;32m   1635\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1636\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/langchain_core/runnables/config.py:347\u001b[0m, in \u001b[0;36mcall_func_with_variable_args\u001b[0;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[1;32m    346\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m run_manager\n\u001b[0;32m--> 347\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/langserve/client.py:335\u001b[0m, in \u001b[0;36mRemoteRunnable._invoke\u001b[0;34m(self, input, run_manager, config, **kwargs)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_invoke\u001b[39m(\n\u001b[1;32m    328\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    329\u001b[0m     \u001b[38;5;28minput\u001b[39m: Input,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    332\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    333\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Output:\n\u001b[1;32m    334\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Invoke the runnable with the given input and config.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 335\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msync_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/invoke\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    337\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m    338\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_lc_serializer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdumpd\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    339\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconfig\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m_prepare_config_for_server\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mkwargs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    342\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    343\u001b[0m     output, callback_events \u001b[38;5;241m=\u001b[39m _decode_response(\n\u001b[1;32m    344\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lc_serializer, response, is_batch\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    345\u001b[0m     )\n\u001b[1;32m    347\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_use_server_callback_events \u001b[38;5;129;01mand\u001b[39;00m callback_events:\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/httpx/_client.py:1145\u001b[0m, in \u001b[0;36mClient.post\u001b[0;34m(self, url, content, data, files, json, params, headers, cookies, auth, follow_redirects, timeout, extensions)\u001b[0m\n\u001b[1;32m   1124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1125\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1126\u001b[0m     url: URLTypes,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1138\u001b[0m     extensions: RequestExtensions \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1139\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Response:\n\u001b[1;32m   1140\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1141\u001b[0m \u001b[38;5;124;03m    Send a `POST` request.\u001b[39;00m\n\u001b[1;32m   1142\u001b[0m \n\u001b[1;32m   1143\u001b[0m \u001b[38;5;124;03m    **Parameters**: See `httpx.request`.\u001b[39;00m\n\u001b[1;32m   1144\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1145\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1146\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPOST\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1147\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1148\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1149\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1150\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfiles\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfiles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1151\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1152\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1153\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1154\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcookies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcookies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1155\u001b[0m \u001b[43m        \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1156\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1157\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextensions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextensions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1159\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/httpx/_client.py:827\u001b[0m, in \u001b[0;36mClient.request\u001b[0;34m(self, method, url, content, data, files, json, params, headers, cookies, auth, follow_redirects, timeout, extensions)\u001b[0m\n\u001b[1;32m    812\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m)\n\u001b[1;32m    814\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuild_request(\n\u001b[1;32m    815\u001b[0m     method\u001b[38;5;241m=\u001b[39mmethod,\n\u001b[1;32m    816\u001b[0m     url\u001b[38;5;241m=\u001b[39murl,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    825\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mextensions,\n\u001b[1;32m    826\u001b[0m )\n\u001b[0;32m--> 827\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/httpx/_client.py:914\u001b[0m, in \u001b[0;36mClient.send\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m    906\u001b[0m follow_redirects \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    907\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfollow_redirects\n\u001b[1;32m    908\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(follow_redirects, UseClientDefault)\n\u001b[1;32m    909\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m follow_redirects\n\u001b[1;32m    910\u001b[0m )\n\u001b[1;32m    912\u001b[0m auth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_request_auth(request, auth)\n\u001b[0;32m--> 914\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    915\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    916\u001b[0m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    918\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    919\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    920\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    921\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/httpx/_client.py:942\u001b[0m, in \u001b[0;36mClient._send_handling_auth\u001b[0;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[1;32m    939\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[1;32m    941\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 942\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    943\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    944\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    945\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    946\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    947\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    948\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/httpx/_client.py:979\u001b[0m, in \u001b[0;36mClient._send_handling_redirects\u001b[0;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[1;32m    976\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequest\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    977\u001b[0m     hook(request)\n\u001b[0;32m--> 979\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    980\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    981\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/httpx/_client.py:1015\u001b[0m, in \u001b[0;36mClient._send_single_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m   1010\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1011\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1012\u001b[0m     )\n\u001b[1;32m   1014\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39mrequest):\n\u001b[0;32m-> 1015\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mtransport\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, SyncByteStream)\n\u001b[1;32m   1019\u001b[0m response\u001b[38;5;241m.\u001b[39mrequest \u001b[38;5;241m=\u001b[39m request\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/httpx/_transports/default.py:232\u001b[0m, in \u001b[0;36mHTTPTransport.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(request\u001b[38;5;241m.\u001b[39mstream, SyncByteStream)\n\u001b[1;32m    220\u001b[0m req \u001b[38;5;241m=\u001b[39m httpcore\u001b[38;5;241m.\u001b[39mRequest(\n\u001b[1;32m    221\u001b[0m     method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[1;32m    222\u001b[0m     url\u001b[38;5;241m=\u001b[39mhttpcore\u001b[38;5;241m.\u001b[39mURL(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    230\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    231\u001b[0m )\n\u001b[0;32m--> 232\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmap_httpcore_exceptions\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/contextlib.py:158\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__exit__\u001b[0;34m(self, typ, value, traceback)\u001b[0m\n\u001b[1;32m    156\u001b[0m     value \u001b[38;5;241m=\u001b[39m typ()\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 158\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgen\u001b[38;5;241m.\u001b[39mthrow(typ, value, traceback)\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m value\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/httpx/_transports/default.py:86\u001b[0m, in \u001b[0;36mmap_httpcore_exceptions\u001b[0;34m()\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m     85\u001b[0m message \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(exc)\n\u001b[0;32m---> 86\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m mapped_exc(message) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n",
      "\u001b[0;31mConnectError\u001b[0m: [Errno 111] Connection refused"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langserve import RemoteRunnable\n",
    "# Invoke generator route --> http://0.0.0.0:9012/generator\n",
    "RemoteRunnable_generator = RemoteRunnable(\"http://0.0.0.0:9012/generator/\")\n",
    "RemoteRunnable_generator.invoke({\"input\": \"Tell me something interesting\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aff364c-519e-435e-bf1d-ce68a12d13e0",
   "metadata": {
    "id": "5aff364c-519e-435e-bf1d-ce68a12d13e0"
   },
   "source": [
    "----\n",
    "\n",
    "<br>\n",
    "\n",
    "## <font color=\"#76b900\">**Congratulations On Completing The Course**</font>\n",
    "\n",
    "Hopefully this course was not only exciting and challenging, but also adequately prepared you for work on the cutting edge of LLM and RAG system development! Going forward, you should have the skills necessary to tackle industry-level challenges and explore RAG deployment with open-source models and frameworks.\n",
    "\n",
    "**Some NVIDIA-specific releases related to this that you may find interesting include:**\n",
    "- [**NVIDIA NIMs**](https://www.nvidia.com/en-us/ai/), which offers microservice spinup routines that can be deployed on local compute.\n",
    "- [**TensorRT-LLM**](https://github.com/NVIDIA/TensorRT-LLM) is the current recommended framework for deploying GPU-accelerated LLM model engines in production settings.\n",
    "- [**NVIDIA's Generative AI Examples Repo**](https://github.com/NVIDIA/GenerativeAIExamples), which includes the current canonical microservice example application and will be updated with new resources as new production workflows get released.\n",
    "- [**The Knowledge-Based Chatbot Technical Brief**](https://resources.nvidia.com/en-us-generative-ai-chatbot-workflow/knowledge-base-chatbot-technical-brief) which discusses additional publicly-accessible details on productionalizing RAG systems.\n",
    "\n",
    "**Additionally, some key topics you may be interested in delving more into include:**\n",
    "- [**LlamaIndex**](https://www.llamaindex.ai/), which has strong components that can augment and occasionally improve upon the LangChain RAG features.\n",
    "- [**LangSmith**](https://docs.smith.langchain.com/), an upcoming agent productionalization service offered by LangChain.\n",
    "- [**Gradio**](https://www.gradio.app/), though touched on in the course, has many more interface options which will be worth investigating. For inspiration, consider checking out [**HuggingFace Spaces**](https://huggingface.co/spaces) for examples.\n",
    "- [**LangGraph**](https://python.langchain.com/docs/langgraph/) is a framework for graph-based LLM orchestration, and is a natural next step forward for those interested in [multi-agent workflows](https://blog.langchain.dev/langgraph-multi-agent-workflows/).\n",
    "- [**DSPy**](https://github.com/stanfordnlp/dspy), a flow engineering framework that allows you to optimize LLM orchestration pipelines based on empirical performance results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035451c9-ed12-4bc3-b468-04db5c399e03",
   "metadata": {
    "id": "035451c9-ed12-4bc3-b468-04db5c399e03"
   },
   "source": [
    "<center><a href=\"https://www.nvidia.com/en-us/training/\"><img src=\"https://dli-lms.s3.amazonaws.com/assets/general/DLI_Header_White.png\" width=\"400\" height=\"186\" /></a></center>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
